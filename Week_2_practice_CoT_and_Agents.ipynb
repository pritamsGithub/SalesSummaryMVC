{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritamsGithub/SalesSummaryMVC/blob/master/Week_2_practice_CoT_and_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to ML from an LLM standpoint\n",
        "# Week 2. Practice session"
      ],
      "metadata": {
        "id": "BjDmRHbLxvre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up\n",
        "\n",
        "Start by loading libraries. And don't forget to enter your OpenAI API key."
      ],
      "metadata": {
        "id": "IzVVOthJpjDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai tiktoken"
      ],
      "metadata": {
        "id": "tjhUBQQ7UkNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "with open(\"openai_api_key\", \"r\") as file:\n",
        "    openai_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "jCUBTtdIRbk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying dialogue roles\n",
        "\n",
        "In this task, we'll work with the [DialogRE benchmark](https://github.com/nlpdata/dialogre) and classify roles in dialogs using Llama models and Nebius AI Studio.\n",
        "\n",
        "Let's start by loading the dataset."
      ],
      "metadata": {
        "id": "E3r-hJxVweIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('dialogre_dev.json', 'r') as f:\n",
        "    dialog_data_raw = json.load(f)"
      ],
      "metadata": {
        "id": "5G2QcVqJk1UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to look at the data."
      ],
      "metadata": {
        "id": "ePkPRnnXxImi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dialog_data_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az0O5hcUpzbb",
        "outputId": "545f2c77-3b89-4cd1-e5c0-f466edf4ed2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "358"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialog_data_raw[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGTbtR10rJPz",
        "outputId": "fb0cb3e5-ac9e-4d43-8b84-76785fc096e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Speaker 1: Hey!',\n",
              "  'Speaker 2: Hey.',\n",
              "  \"Speaker 3: Hey, man. What's up?\",\n",
              "  \"Speaker 1: Maybe you can tell me. My agent would like to know why I didn't show up at the audition I didn't know I had today. The first good thing she gets me in weeks. How could you not give me the message?!\",\n",
              "  \"Speaker 3: Well, I'll tell ya I do enjoy guilt, but, ah, it wasn't me.\",\n",
              "  'Speaker 2: Yes, it was! It was him! Uh huh! Okay, it was me!',\n",
              "  'Speaker 1: How is it you?',\n",
              "  \"Speaker 2: Well, it was just, it was all so crazy, you know. I mean, Chandler was in the closet, counting to 10, and he was up to 7 and I hadn't found a place to hide yet. I-I-I meant to tell you, and I wrote it all down on my hand. See, all of it.\",\n",
              "  \"Speaker 1: Yep, that's my audition.\",\n",
              "  'Speaker 4: See, now this is why I keep notepads everywhere.',\n",
              "  \"Speaker 2: Yep, and that's why we don't invite you to play.\",\n",
              "  'Speaker 5: What is the great tragedy here? You go get yourself another appointment.',\n",
              "  'Speaker 1: Well, Estelle tried, you know. The casting director told her that I missed my chance.',\n",
              "  \"Speaker 2: That is unfair. I'll call her and tell her it was totally my fault.\",\n",
              "  \"Speaker 1: Pheebs, you can't do that. The casting director doesn't talk to friends, she only talks to agents.\",\n",
              "  'Speaker 2: What a sad little life she must lead. Okay, ooh.',\n",
              "  'Speaker 1: What, what are you doing? What are you doing?',\n",
              "  \"Speaker 2: No, no, no, I know, I know, ooh. 'Hi, this is Katelynn, from Phoebe Buffay's office. Um, is um, Ann there for Phoebe, she'll know what it's about.'\",\n",
              "  'Speaker 1: Hang up, hang up.',\n",
              "  \"Speaker 2: 'Annie! Hi. Listen we got a problem with Joey Tribbiani, apparently he missed his audition. Who did you speak to in my office? Estelle, no, I don't know what I'm going to do with her. No. All right, so your husband leaves and burns down the apartment, the world does not stop.'\",\n",
              "  'Speaker 3: Is anybody else scared?',\n",
              "  \"Speaker 2: 'Right, well look, um, if Joey loses this audition, that is it for Estelle. I don't care! Annie you are a doll, what time can you see him?' I need a pen.\",\n",
              "  'Speaker 3: Get the woman a pad! Get the woman a pad! A pad! A pad!',\n",
              "  'Speaker 4: Oh, now you want a pad.'],\n",
              " [{'y': 'casting director',\n",
              "   'x': 'Ann',\n",
              "   'rid': [29],\n",
              "   'r': ['per:title'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'STRING'},\n",
              "  {'y': 'Annie',\n",
              "   'x': 'Ann',\n",
              "   'rid': [30],\n",
              "   'r': ['per:alternate_names'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'agent',\n",
              "   'x': 'Estelle',\n",
              "   'rid': [29],\n",
              "   'r': ['per:title'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'STRING'},\n",
              "  {'y': 'Speaker 1',\n",
              "   'x': 'Estelle',\n",
              "   'rid': [7],\n",
              "   'r': ['per:client'],\n",
              "   't': ['agent'],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'Katelynn',\n",
              "   'x': 'Speaker 2',\n",
              "   'rid': [30],\n",
              "   'r': ['per:alternate_names'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'Pheebs',\n",
              "   'x': 'Speaker 2',\n",
              "   'rid': [30],\n",
              "   'r': ['per:alternate_names'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'Speaker 1',\n",
              "   'x': 'Speaker 2',\n",
              "   'rid': [9],\n",
              "   'r': ['per:friends'],\n",
              "   't': ['friends'],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'Phoebe Buffay',\n",
              "   'x': 'Speaker 2',\n",
              "   'rid': [30],\n",
              "   'r': ['per:alternate_names'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'casting director',\n",
              "   'x': 'Annie',\n",
              "   'rid': [29],\n",
              "   'r': ['per:title'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'STRING'},\n",
              "  {'y': 'man',\n",
              "   'x': 'Speaker 3',\n",
              "   'rid': [37],\n",
              "   'r': ['unanswerable'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'STRING'},\n",
              "  {'y': 'man',\n",
              "   'x': 'Speaker 1',\n",
              "   'rid': [30],\n",
              "   'r': ['per:alternate_names'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'STRING'},\n",
              "  {'y': 'Speaker 2',\n",
              "   'x': 'Speaker 1',\n",
              "   'rid': [9],\n",
              "   'r': ['per:friends'],\n",
              "   't': ['friends'],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'},\n",
              "  {'y': 'Joey Tribbiani',\n",
              "   'x': 'Speaker 1',\n",
              "   'rid': [30],\n",
              "   'r': ['per:alternate_names'],\n",
              "   't': [''],\n",
              "   'x_type': 'PER',\n",
              "   'y_type': 'PER'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, many different character roles are labeled for every dialog.\n",
        "\n",
        "Here is the list of all roles:"
      ],
      "metadata": {
        "id": "5cTuF37NxMWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_relations = []\n",
        "for dialog in dialog_data_raw:\n",
        "    for relation in dialog[1]:\n",
        "        for individual_relation in relation['r']:\n",
        "            if not individual_relation in all_relations:\n",
        "                all_relations.append(individual_relation)\n",
        "all_relations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIZKSCiQpedG",
        "outputId": "bd633f5b-77fe-4604-a2e1-05bef848c009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['per:title',\n",
              " 'per:alternate_names',\n",
              " 'per:client',\n",
              " 'per:friends',\n",
              " 'unanswerable',\n",
              " 'per:spouse',\n",
              " 'per:children',\n",
              " 'per:parents',\n",
              " 'per:age',\n",
              " 'per:siblings',\n",
              " 'per:roommate',\n",
              " 'per:negative_impression',\n",
              " 'per:pet',\n",
              " 'per:positive_impression',\n",
              " 'per:girl/boyfriend',\n",
              " 'org:employees_or_members',\n",
              " 'per:employee_or_member_of',\n",
              " 'per:dates',\n",
              " 'per:boss',\n",
              " 'per:subordinate',\n",
              " 'per:other_family',\n",
              " 'org:students',\n",
              " 'per:major',\n",
              " 'per:schools_attended',\n",
              " 'per:origin',\n",
              " 'gpe:visitors_of_place',\n",
              " 'per:visited_place',\n",
              " 'per:alumni',\n",
              " 'per:works',\n",
              " 'per:place_of_residence',\n",
              " 'gpe:residents_of_place',\n",
              " 'per:place_of_work',\n",
              " 'per:date_of_birth',\n",
              " 'per:acquaintance',\n",
              " 'per:neighbor',\n",
              " 'gpe:births_in_place',\n",
              " 'per:place_of_birth']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't use all roles, only the following ones:"
      ],
      "metadata": {
        "id": "mhSwSYUaxT07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_relations = [\n",
        "    'per:friends', 'per:spouse', 'per:children', 'per:parents',\n",
        "    'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate'\n",
        "]"
      ],
      "metadata": {
        "id": "5BlD8YlZqL8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialog_data = []\n",
        "for dialog in dialog_data_raw:\n",
        "    current_dialog = dialog[0]\n",
        "    current_relations = []\n",
        "    for relation in dialog[1]:\n",
        "        if relation['r'][0] in our_relations:\n",
        "            current_relations.append({\n",
        "                'x': relation['x'],\n",
        "                'y': relation['y'],\n",
        "                'r': relation['r'][0]\n",
        "            })\n",
        "    if len(current_relations) > 0:\n",
        "        dialog_data.append([current_dialog, current_relations])"
      ],
      "metadata": {
        "id": "ZI4bBhfWqtp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dialog_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVOFNT81rYw-",
        "outputId": "6aef25a5-948e-463d-ccbc-df75ff9aecd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialog_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsmW12oyrcCx",
        "outputId": "a66116fd-b3f7-45ce-a88c-81a46598a139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Speaker 1, Speaker 2: Hi',\n",
              "  'Speaker 3: Hi! Hey mom.',\n",
              "  'Speaker 4: This is such a great party! 35 years. Very impressive, do you guys have any pearls of wisdom?',\n",
              "  'Speaker 2: Jack?',\n",
              "  'Speaker 1: Why would you serve food on such a sharp stick?',\n",
              "  'Speaker 3: That’s a good question, dad. That’s a good question…',\n",
              "  'Speaker 4: Hmmm….'],\n",
              " [{'x': 'Speaker 1', 'y': 'Speaker 2', 'r': 'per:spouse'},\n",
              "  {'x': 'Speaker 1', 'y': 'Speaker 3', 'r': 'per:children'},\n",
              "  {'x': 'Speaker 2', 'y': 'Speaker 1', 'r': 'per:spouse'},\n",
              "  {'x': 'Speaker 2', 'y': 'Speaker 3', 'r': 'per:children'},\n",
              "  {'x': 'Jack', 'y': 'Speaker 3', 'r': 'per:children'},\n",
              "  {'x': 'Speaker 3', 'y': 'Speaker 2', 'r': 'per:parents'},\n",
              "  {'x': 'Speaker 3', 'y': 'Speaker 1', 'r': 'per:parents'},\n",
              "  {'x': 'Speaker 3', 'y': 'Jack', 'r': 'per:parents'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, we will be predicting a role for only one pair of character from each dialog.\n",
        "And to save time and money, we'll only take 50 first dialogues from the `dev` set."
      ],
      "metadata": {
        "id": "mPY3PS2FxZP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(28)\n",
        "dialog_data_short = [[dialog, np.random.choice(relations)] for dialog, relations in dialog_data[:50]]"
      ],
      "metadata": {
        "id": "4d1_v0BvrrNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialog_data_short[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmPyshacswcP",
        "outputId": "d198f329-5628-42d7-cdd9-77aaa1913ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Speaker 1, Speaker 2: Hi',\n",
              "  'Speaker 3: Hi! Hey mom.',\n",
              "  'Speaker 4: This is such a great party! 35 years. Very impressive, do you guys have any pearls of wisdom?',\n",
              "  'Speaker 2: Jack?',\n",
              "  'Speaker 1: Why would you serve food on such a sharp stick?',\n",
              "  'Speaker 3: That’s a good question, dad. That’s a good question…',\n",
              "  'Speaker 4: Hmmm….'],\n",
              " {'x': 'Speaker 1', 'y': 'Speaker 3', 'r': 'per:children'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_true = [relations['r'] for _, relations in dialog_data_short]"
      ],
      "metadata": {
        "id": "iCb35mrt3NZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also important to look at the target label distribution. In our case it's not balanced: there are much more friends and girl/boyfriends than other roles."
      ],
      "metadata": {
        "id": "h_xwkLtOxmUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "relations_counter = Counter(verdicts_true)\n",
        "\n",
        "relations_counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LLkoKYsYuXt",
        "outputId": "e76c0c37-aa04-4acb-ad8b-304ee779d4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'per:friends': 11,\n",
              "         'per:children': 3,\n",
              "         'per:parents': 4,\n",
              "         'per:siblings': 6,\n",
              "         'per:spouse': 3,\n",
              "         'per:girl/boyfriend': 16,\n",
              "         'per:boss': 5,\n",
              "         'per:subordinate': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a deep breath and work out this problem step by step (Chain-of-Thought prompting / Reasoning)\n",
        "\n",
        "This is a magic phrase that was used back in 2023 to make LLMs produce not only answers but full solutions. it quickly became clear that in solution-writing mode LLMs are more likely to produce correct answers, so today's models are all trained to provide reasoning. (You'd actually have to convince an LLM to provide only answer without a solution.)\n",
        "\n",
        "Now, let's create an LLM-based classifier for our task."
      ],
      "metadata": {
        "id": "SJUF5oA6s8lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelationClassifier():\n",
        "    def __init__(self, client, model):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, dialog, character_x, character_y, verbose=False):\n",
        "        completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"You are an expert in Natural Nanguage Understanding.\n",
        "You are gived a dialog and two characters participating or mentioned in this dialog.\n",
        "You need to predict relationships between these characters choosing from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "Provide a clear reasoning justifying your choice.\n",
        "\n",
        "DIALOG: {dialog}\n",
        "\n",
        "FIRST CHARACTER: {character_x}\n",
        "\n",
        "SECOND CHARACTER: {character_y}\n",
        "\n",
        "ANSWER\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"completion\": completion,\n",
        "                \"answer\": completion.choices[0].message.content\n",
        "            }\n",
        "        else:\n",
        "            return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "acjWi6mGs8l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use this classifier."
      ],
      "metadata": {
        "id": "d2C2AQozs8l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "classifier_cot = RelationClassifier(client=client, model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\")"
      ],
      "metadata": {
        "id": "FNmCHFM_s8l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tqdm library allows to create progress bars for cycles\n",
        "from tqdm import tqdm\n",
        "\n",
        "verdicts_raw = []\n",
        "for dialog, relations in tqdm(dialog_data_short[:5]):\n",
        "    verdicts_raw.append(classifier_cot.predict(dialog, relations['x'], relations['y']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6608e8-ac16-4896-ecbb-54755ed0b60f",
        "id": "aYxawxROs8l6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:36<00:00,  7.22s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the answers:"
      ],
      "metadata": {
        "id": "Oi6DD8b1s8l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3d72c0-4269-4f0a-f4c9-fa4feff65752",
        "id": "WOa04h4ts8l6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The relationship between the two characters is: \\'per:friends\\'.\\n\\nReasoning:\\n \\n* The tone of the conversation between the two characters is casual and joking, indicating a friendly relationship.\\n* They engage in playful banter, with Speaker 1 becoming frustrated but Speaker 2 attempting to make light of the situation and offer a solution.\\n* They appear to be comfortable with each other, with Speaker 2 teasing Speaker 1 and Speaker 1 being exasperated but ultimately good-natured about it.\\n* Speaker 2 is willing to go out of their way to help Speaker 1, calling the casting director and pretending to be someone from a different office in order to secure another audition for Speaker 1. This indicates a level of commitment and loyalty to their friendship.\\n* The fact that they are socializing and playing a game (as indicated by Speaker 4\\'s statement \"Now you want a pad\" and Speaker 2\\'s earlier statement \"and that\\'s why we don\\'t invite you to play\") suggests that they are friends who spend time together. \\n\\nOverall, the tone and content of the conversation between Speaker 1 and Speaker 2 are consistent with a friendly relationship.',\n",
              " 'Based on the given dialog, I predict the relationship between Speaker 1 and Speaker 3 as \\'per:parents\\' (specifically, Speaker 1 is the parent and Speaker 3 is the child).\\n\\nMy justification is as follows:\\n\\n1. In the dialog, Speaker 3 says \"Hey mom\" initially, but later addresses Speaker 1 as \"dad\". This confirms that Speaker 1 is indeed the parent of Speaker 3.\\n2. The familiar tone and language used by Speaker 3 when addressing Speaker 1 (\"Hey mom\", \"dad\") suggests a close familial relationship.\\n3. The fact that Speaker 3 immediately jumps into a conversation between Speaker 1 and others, questioning Speaker 1\\'s food-serving choices, also implies a level of comfort and familiarity that is typical of a parent-child relationship.\\n\\nTherefore, the most likely relationship between Speaker 1 and Speaker 3 is \\'per:parents\\'.',\n",
              " 'Based on the dialog provided, the relationship between Speaker 3 and Emma is likely \\'per:parents\\' or more specifically, \\'per:father\\' or \\'per:mother\\', but since \\'per:parents\\' is the available option, I\\'ll choose it.\\n\\nMy reasoning is as follows:\\n1. Speaker 3 says \"There\\'s something you didn\\'t know about your dad!\" - This line indicates that Speaker 3 has some familiarity with Emma\\'s family and seems to be someone close to her, making a familial relationship plausible.\\n\\n2. Emma is mentioned as being one year old and having a birthday party where Speaker 3, along with other characters ( Speaker 1, Speaker 2), seem to be interacting with Mr. and Mrs. Geller. Given the situation (a birthday party with family friends), Speaker 3 could be one of Emma\\'s parents.\\n\\nHowever, please note that without explicit information or additional details, we can only make educated guesses about their relationship. Given the context, I would predict their relationship to be \\'per:parents\\'.',\n",
              " \"Based on the given dialog, I predict the relationship between Speaker 2 and Speaker 3 as 'per:friends' or more specifically roommates, but since 'per:roommate' is not available, 'per:friends' seems a suitable option, but more suitable would be per:siblings or per:spouse as they all live together.\\n\\nHowever, the last line, Speaker 3 says 'Hi Ross!' and this name 'Ross' has been used by Speaker 1 before being called by Speaker 3. Hence, Speaker 1 is Ross and when Speaker 3 says 'Hi Ross',  it means Speaker 3 is greeting Speaker 1 (Ross), the husband of Speaker 2 as Speaker 2 said 'She’s gonna help us take care of the baby' \\n\\nTherefore, my prediction is that Speaker 2 and Speaker 3 are 'per:friends'\",\n",
              " 'For the relationship between Speaker 1 and Ross, I predict: \\'per:girl/boyfriend\\'\\n\\nReasoning: The Speaker 1 is having a conversation with their mom, discussing Ross, and is somewhat defensive, indicating a close, possibly intimate relationship. Speaker 1 mentions Ross is in \"a whole other place\", suggesting Ross may be going through a personal or emotional time. Additionally, when the mom mentions something about babies, Speaker 1 responds quickly and defensively, saying they are \"not even thinking about babies yet\", suggesting the Speaker\\'s relationship with Ross could be in a place where marriage or children are potential considerations. The way the Speaker is defending their independence and explaining their feelings to their mom implies that the Speaker and Ross have a romantic relationship, which makes \\'per:girl/boyfriend\\' a suitable choice.\\n\\n\\nBetween Speaker 1 and their mom ( Ross not directly participates,  but seems mentioned as related to Speaker 1. Hence, let let make prediction for mom although such not asked):  (will choice for compteness of undestanding of provided Diaog between given characters relationship status) \\n\\nAdditionally, for the relationship between Speaker 1 and their mom, I predict: \\'per:parents\\'.\\n\\nReasoning: Speaker 1 explicitly refers to the person they\\'re talking to as \"Mom\", making this prediction straightforward. The speaker\\'s tone and responses, including reassurance and some defensive explanations, also align with the sort of interactions one might expect between a parent and their child.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answers seem to be correct, but their format is random, and extracting answers from them is not too simple. We'll actually need to use another LLM call for that!"
      ],
      "metadata": {
        "id": "9csA23Bws8l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chaining\n",
        "\n",
        "Chaining refers to using several LLMs sequentially. In our case, we'll have a very simple chain:\n",
        "\n",
        "```\n",
        "LLM_1: complaint -> reasoning\n",
        "LLM_2: reasoning -> answer\n",
        "```"
      ],
      "metadata": {
        "id": "vVfocAo-uIYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RelationClassifierChain():\n",
        "    def __init__(self, client, model):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, dialog, character_x, character_y, verbose=False):\n",
        "        reasoning_completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"You are an expert in Natural Nanguage Understanding.\n",
        "You are gived a dialog and two characters participating or mentioned in this dialog.\n",
        "You need to predict relationships between these characters choosing from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "Provide a clear reasoning justifying your choice. Then write your final answer after VERDICT.\n",
        "Now, take a deep breath and work out this problem step by step. If you do well, I'll tip you 200$.\n",
        "\n",
        "DIALOG: {dialog}\n",
        "\n",
        "FIRST CHARACTER: {character_x}\n",
        "\n",
        "SECOND CHARACTER: {character_y}\n",
        "\n",
        "REASONING:\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "        reasoning = reasoning_completion.choices[0].message.content\n",
        "\n",
        "        extraction_completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"You are a helpful assistant.\n",
        "You are given the following reasoning which justifies a verdict from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "Extract the verdict. Only output the verdict from the list.\n",
        "\n",
        "REASONING: {reasoning}\n",
        "\n",
        "VERDICT: \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "\n",
        "        verdict = extraction_completion.choices[0].message.content\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"reasoning_completion\": reasoning_completion,\n",
        "                \"extraction_completion\": extraction_completion,\n",
        "                \"verdict\": verdict\n",
        "            }\n",
        "        else:\n",
        "            return verdict"
      ],
      "metadata": {
        "id": "KrqQpYFruIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_chain = RelationClassifierChain(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "    )"
      ],
      "metadata": {
        "id": "5OStYwWyuIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's good to start logging the results"
      ],
      "metadata": {
        "id": "7Yinf3RNuIYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completions_log = dict() # Raw completions\n",
        "verdicts_log = dict() # Final verdicts"
      ],
      "metadata": {
        "id": "zim7LWvFuIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tqdm library allows to create progress bars for cycles\n",
        "from tqdm import tqdm\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-405B-Instruct, chain\"\n",
        "results = []\n",
        "# do it for patient_visits[-10:] to save time\n",
        "for dialog, relations in tqdm(dialog_data_short):\n",
        "    results.append(classifier_chain.predict(dialog, relations['x'], relations['y'], verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442c84f0-bea5-49d8-ce57-86a3fadfc479",
        "id": "PBVR38n5uIYx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [08:53<00:00, 10.66s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the results:"
      ],
      "metadata": {
        "id": "i13-Tjo7uIYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfff34ff-549b-43f2-9878-d367a145b845",
        "id": "QJ0vcnp2uIYx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning_completion': ChatCompletion(id='chat-96edbd003f1c489f93aa2d75dd2a958f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To determine the relationship between Speaker 1 and Speaker 3, let\\'s analyze the dialog provided.\\n\\n1. Speaker 3 greets with \"Hi! Hey mom.\" Initially, this could suggest that Speaker 3 is addressing their mother, but since Speaker 3 later addresses Speaker 1 as \"dad,\" we can infer that Speaker 3 was addressing their mother who is not participating in this part of the conversation or is Speaker 4. It doesn\\'t impact our main relationship in question.\\n\\n2. The critical clue is when Speaker 3 says, \"That’s a good question, dad. That’s a good question…\" This line directly addresses Speaker 1 as \"dad,\" which indicates a parental relationship.\\n\\nGiven these points, the relationship between Speaker 1 and Speaker 3 can be classified under the \\'per:parents\\' category since Speaker 1 is the parent (father) of Speaker 3.\\n\\nVERDICT: \\'per:parents\\'', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731333358, model='meta-llama/Meta-Llama-3.1-405B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=193, prompt_tokens=289, total_tokens=482, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'extraction_completion': ChatCompletion(id='chat-7bf1a74704a24436ab08bfa79b883790', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"'per:parents'\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731333364, model='meta-llama/Meta-Llama-3.1-405B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=314, total_tokens=320, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'verdict': \"'per:parents'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62dc10a-c264-43a4-b698-ae61c9a43aef",
        "id": "t5GKTBS0uIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['per:friends', \"'per:parents'\", \"'per:parents'\", 'per:friends', 'per:spouse', 'per:friends', 'per:girl/boyfriend', \"'per:boss'\", 'per:spouse', 'per:girl/boyfriend', 'per:boss', 'per:girl/boyfriend', 'per:subordinate', 'per:girl/boyfriend', 'per:girl/boyfriend', 'per:boss', 'per:friends', 'per:girl/boyfriend', 'per:spouse', 'per:friends', 'per:friends', 'per:girl/boyfriend', 'per:girl/boyfriend', 'per:friends', 'per:girl/boyfriend', 'per:siblings', 'per:boss', 'per:friends', 'per:parents', 'per:girl/boyfriend', 'per:friends', 'per:friends', 'per:spouse', 'per:friends', 'per:siblings', \"'per:girl/boyfriend'\", \"'per:siblings'\", 'per:girl/boyfriend', 'per:parents', 'per:friends', 'per:friends', 'per:girl/boyfriend', 'per:siblings', 'per:girl/boyfriend', 'per:girl/boyfriend', 'per:parents', 'per:children', 'per:boss', 'per:subordinate', 'per:friends']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a slight format inconsistency, but it can be helped with elementary postprocessing."
      ],
      "metadata": {
        "id": "2RIGMIMZuIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = [verdict.strip('\"').strip(\"'\") for verdict in verdicts_raw]\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "FMiHDA_MuIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check the accuracy of our predictions."
      ],
      "metadata": {
        "id": "QUs5ryKquIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
        "\n",
        "accuracy_score(verdicts_true, verdicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf3d595-9ce5-4cad-ba9c-ec889ddbeb28",
        "id": "04DLp75NuIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.74"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's not bad for a start!"
      ],
      "metadata": {
        "id": "Z2vVSd-kuIYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A smaller LLM"
      ],
      "metadata": {
        "id": "Zxb5AMVduIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_chain_8b = RelationClassifierChain(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    )\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-8B-Instruct, chain\"\n",
        "results = []\n",
        "# do it for patient_visits[-10:] to save time\n",
        "for dialog, relations in tqdm(dialog_data_short):\n",
        "    results.append(classifier_chain_8b.predict(dialog, relations['x'], relations['y'], verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c974f3f4-2b25-478c-8a1c-7b89026d7f12",
        "id": "C8e_7vrCuIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [04:13<00:00,  5.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[-3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f282ad2-ccb9-414e-a86a-1cb1c7c8908a",
        "id": "h7ccwuXguIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning_completion': ChatCompletion(id='chat-934dd724a2814e11847567c550d93a25', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Based on the dialog, I predict the relationship between Mr. Kostelick and Speaker 2 as \\'per:boss\\'.\\n\\nReasoning:\\n\\n1. Speaker 2 is mentioned as needing to stop by Mr. Kostelick\\'s office at the end of the day (Speaker 3\\'s line).\\n2. Mr. Kostelick is referred to as \"Mr. Kostelick wants you to stop by his office at the end of the day\" (Speaker 3\\'s line), indicating that Speaker 2 reports to or is supervised by Mr. Kostelick.\\n3. Speaker 2 is also defensive when accused of being involved in a prank memo, and mentions \"Mr. Kostelick\" in the context of work (Speaker 2\\'s lines \"If this is about those prank memos, I had nothing to do with them.\").\\n4. Speaker 7, Chandler, mentions that Speaker 2 has been at the office for five years, implying a long-standing employment relationship.\\n5. Speaker 2\\'s lines suggest that they have a certain level of familiarity with Mr. Kostelick\\'s office and working style, including the fact that someone like \"big Al\" (presumably a reference to a previous manager) was able to make them a job offer.\\n\\nGiven these points, it appears that Mr. Kostelick is Speaker 2\\'s supervisor or boss.\\n\\nVERDICT:\\n\\n\\'per:boss\\'', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731334121, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=289, prompt_tokens=488, total_tokens=777, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'extraction_completion': ChatCompletion(id='chat-2cbb52b2c68b4e038966ba6b0fc81ac1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"'per:boss'\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731334126, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=385, total_tokens=391, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'verdict': \"'per:boss'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf2c4e84-f6c8-4378-d37e-ac0d68714f81",
        "id": "bStA382nuIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'per:client'\", \"'per:parents'.\", '**per:parents**', \"'per:friends'\", 'Ross is the boyfriend or partner of Speaker 1.', \"['per:friends']\", 'per:spouse', 'per:boss', \"'per:girl/boyfriend'\", \"'per:friends'\", \"['per:subordinate']\", 'per:girl/boyfriend', \"['per:boss']\", 'per:spouse', \"['per:friends']\", '\\'per:girl/boyfriend\\' is unlikely, but... \\'per:girl/boyfriend\\' is not the final verdict\\n\\nWAIT, MY DIAGNOSTIC IS \\'per:girl/boyfriend\\' UNLIKELY BUT...\\n\\nMy diagnosis is \\'per:girl/boyfriend\\' unlikely, but the final verdict is likely one of the remaining options which is:\\n\\n\\'per:girl/boyfriend\\' is unlikely but another option is more likely\\n\\nThe final verdict is:\\n\\'per:girl/boyfriend\\' is unlikely but another option is more likely.\\n\\nHowever, there is more text that indicates the verdict.\\n\\n* \\'per:girl/boyfriend\\'- unlikely, as the dialogue does not suggest a romantic relationship.\\n* \\'per:subordinate\\' - unlikely, as Speaker 2 seems to be on an equal level or possibly higher than Speaker 1.\\n\\nHowever, \\'per:girl/boyfriend is unlikely\\' but another option is more likely. Considering other possibilities, the most likely relationship is as follows.\\n\\nBased on the text, the actual verdict is:\\n\\n\\'per:friens\\' is possible but not the best match, \\'per:boss\\', is possible but eliminated, \\'per:girl/boyfriend\\' is unlikely but \\'per:siblings\\', \\'per:children\\', \\'per:parents\\' are all unlikely and we are also not considering \\'per:subordinate\\' because the relationship is likely more equal or possibly higher.\\n\\nHowever, what we are left with is \\'per:friend\\', that could potentially be correct given the use of \"Pheebs\" and getting scolded for getting involved with assistants.\\n\\nGiven the information above the verdict is:\\n\\n\\'per:friends\\'', '**per:friends**', \"'per:girl/boyfriend'\", 'per:spouse', 'per:friends', \"'per:friends'\", \"**'per:girl/boyfriend'**\", 'per:girl/boyfriend', \"'per:friends'\", '*per:spouse, per:girl/boyfriend*', \"'per:siblings'\", \"'per:boss'\", '**per:friends**', 'per:children between Susan and Ben.', \"'per:girl/boyfriend'\", \"['per:friends']\", \"I'd be happy to help!\\n\\n'per:friends'\", 'The verdict is: Mattress King and Janice are spousal figures.', \"'per:friends'\", \"['per:siblings']\", '**per:girl/boyfriend**', \"'per:friends'\", \"['per:spouse', 'per:former partner']\", \"'per:parents'\", \"['per:friends']\", '**per:friends**', 'per:girl/boyfriend', 'per:children', 'per:girl/boyfriend', \"['per:girl/boyfriend']\", 'per:parents', 'per:friends', \"'per:boss'\", \"['per:boss', 'per:subordinate']\", 'per:friends']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = [verdict.strip('[').strip(']').strip('\"').strip(\"'\") for verdict in verdicts_raw]\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "ZH4xWAqmuIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(verdicts_true, verdicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a931e941-5a33-46d2-81f3-f873d5288e49",
        "id": "-e0Y2ck0uIYy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's worse.\n",
        "\n",
        "But let's also check how many of the raw verdict are in the correct format:"
      ],
      "metadata": {
        "id": "JVTyIZ6wuIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([(raw_verdict in our_relations) for raw_verdict in verdicts_raw]) / len(verdicts_raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLIhNFOWNmdE",
        "outputId": "6b3664bb-7f94-4888-ceaa-588c5d742aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That could be better, and we'll try to adress this with few-shot learning!"
      ],
      "metadata": {
        "id": "iwXvYPnDcqvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot examples: show, don't tell!\n",
        "\n",
        "A very cool feature of LLMs is **In-context learning**. One of its manifestations is the ability of \"learn\" on examples put into a prompt.\n",
        "\n",
        "Like this:\n",
        "\n",
        "```\n",
        "Context + Task description\n",
        "\n",
        "USER: {example 1}\n",
        "\n",
        "ASSISTANT: {solution for example 1}\n",
        "###\n",
        "USER: {example 2}\n",
        "\n",
        "ASSISTANT: {solution for example 2}\n",
        "###\n",
        "USER: {real task}\n",
        "\n",
        "ASSISTANT:\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "FVjUmqJ36nG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's choose several ground truth examples (not from the validation set!) and include them as few-shot examples to makes things better in terms of output format at least."
      ],
      "metadata": {
        "id": "qXm5n7Yo6nHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_reasonings = []\n",
        "for i, (dialog, relations) in tqdm(enumerate([[dialog, relations[0]] for dialog, relations in dialog_data[50:60]])):\n",
        "    result = classifier_chain_8b.predict(dialog, relations['x'], relations['y'], verbose=True)\n",
        "    few_shot_reasonings.append(result['reasoning_completion'].choices[0].message.content)\n",
        "    print(result['reasoning_completion'].choices[0].message.content)\n",
        "    print(f\"\\n###\\nThat was dialog number {i} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YPc13kYPJ1D",
        "outputId": "14b8770d-606a-4de7-d0ab-75fe7330913a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:06,  6.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the dialog, I can infer the relationship between Speaker 1 and Speaker 2 as follows:\n",
            "\n",
            "The conversation between Speaker 1 and Speaker 2 seems to be quite intimate and emotionally charged. The fact that Speaker 1 explicitly states \"I am your friend\" suggests that they have a close relationship, but the tone and content of the conversation suggest that it's more than a casual friendship.\n",
            "\n",
            "The mention of a \"relationship\" being over implies that they were in a romantic relationship previously. Speaker 1 seems to be contemplating the end of their relationship, and Speaker 2 is expressing sadness and longing. Speaker 2's repeated requests for juice can be seen as a metaphor for a deeper emotional need, which is often associated with romantic partners.\n",
            "\n",
            "Furthermore, the use of affectionate language, such as \"Look, man\" and \"Okay, man?\", suggests a level of familiarity and closeness that is typical of romantic partners.\n",
            "\n",
            "Based on this analysis, I believe the relationship between Speaker 1 and Speaker 2 is most likely:\n",
            "\n",
            "**VERDICT**\n",
            "\n",
            "* Relationship: per:spouse\n",
            "\n",
            "I chose this option because the conversation implies a deep emotional connection and a history of a romantic relationship between the two characters. While it's possible that they could be close friends or family members, the tone and language used suggests a more intimate and romantic connection.\n",
            "\n",
            "###\n",
            "That was dialog number 0 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:09,  4.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's break down the relationships between the characters mentioned in the dialog.\n",
            "\n",
            "From the conversation, we know that:\n",
            "- Rachel is the person Ross is trying to make up with.\n",
            "- Phoebe is friends with Rachel.\n",
            "\n",
            "Also, Phoebe works at the same massage place as Jasmine, who is Issac's sister. However, they are not directly related to each other, so we don't have enough information to infer a specific relationship amongst them yet.\n",
            "\n",
            "As for Rachel and Phoebe, and Ross and Rachel, and Phoebe and her friends, they seem to be connected through various degrees of friendship and relationships, but not necessarily a romantic relationship.\n",
            "\n",
            "Given the context of the conversation between Ross, Joey (the speakers), and Chandler (speaker 3), it seems they are Ross's friends.\n",
            "\n",
            "###\n",
            "That was dialog number 1 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:15,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To determine the relationship between Speaker 2 and Speaker 3, I'll analyze the dialog and look for clues that hint at their connection.\n",
            "\n",
            "The key point to focus on is the revelation by Speaker 1 that he and Speaker 2 shared a kiss with Rachel in the same night. This leads to an exchange between Speaker 2 and Speaker 3, where they start to piece together what happened:\n",
            "\n",
            "* Speaker 2: \"Seriously, where did this happen?\"\n",
            "* Speaker 1: \"Okay, after you told me she was passed out in our room, I went in there to make sure she was all right.\"\n",
            "* Speaker 3: \"I'm pretty sure I put her on my bed.\"\n",
            "\n",
            "Given this context, it's clear that Speaker 1 and Speaker 3 are trying to figure out what happened between them and Speaker 2 and Rachel. The confusion about who was on whose bed indicates that they are often in the same social circle and may have had interactions before.\n",
            "\n",
            "Moreover, the ending lines hint at a deeper connection:\n",
            "\n",
            "* Speaker 3: \"What did I marry into?\"\n",
            "* Speaker 2: (smiling) While the exact nature of the relationship is not explicitly stated, the playful and loving tone of Speaker 2's expression (as inferred from the transcription) and the \"idea\" of being married, mean they likely have a significant collaborative or family unit-based relationship.\n",
            "\n",
            "Considering these points, the most likely relationship between Speaker 2 and Speaker 3 is:\n",
            " \n",
            "['per:spouse']\n",
            "\n",
            "This relationship explains the shared knowledge, apparent familiarity, and the level of intimacy implied throughout the conversation.\n",
            "\n",
            "VERDICT:\n",
            "per:spouse\n",
            "\n",
            "###\n",
            "That was dialog number 2 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [00:20,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided dialog and characters, I'll analyze the relationships between Monica, Chandler, Speaker 1, and Speaker 2.\n",
            "\n",
            "From the dialog, it's clear that:\n",
            "\n",
            "* Monica and Chandler are mentioned in conjunction with each other (\"Monica and Chandler almost got married\"). This suggests a romantic relationship.\n",
            "* Speaker 2 mentions marrying Chandler for the money, which implies that Chandler has financial value in their relationship.\n",
            "* The dialogue also mentions Chandler's hypothetical wealth, which further suggests that Chandler is in a relationship with either Monica or the speaker.\n",
            "* Speaker 2 mentions marrying Chandler, which eliminates the possibility that Speaker 1 and Speaker 2 are in a romantic relationship. The only relationship that makes sense is between Speaker 2 and Chandler.\n",
            "\n",
            "Given these points, I will choose the following relationships:\n",
            "\n",
            "* per:siblings: Ross is mentioned in the dialog, and Monica is his sister. This is a likely relationship.\n",
            "* per:girl/boyfriend: Chandler and Monica are in a romantic relationship.\n",
            "* per:spouse: Monica and Chandler are in a committed relationship.\n",
            "\n",
            "Considering these relationships, my prediction is:\n",
            "\n",
            "Speaker 2: Chandler\n",
            "Speaker 1: Monica\n",
            "\n",
            "VERDICT:\n",
            "\n",
            "per:siblings: Ross and Monica\n",
            "per:girl/boyfriend: Chandler and Monica\n",
            "(per:spouse: Monica and Chandler) \n",
            "\n",
            "PAID\n",
            "\n",
            "###\n",
            "That was dialog number 3 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [00:24,  4.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the given dialog, I'll analyze the relationship between Speaker 3 and Speaker 1.\n",
            "\n",
            "Speaker 3 and Speaker 1 are both shown to be advising and discussing the situation between Speaker 1 and Joey. Speaker 3 says \"and as his friend, I mean, don't you think he deserves the same from you?\", which indicates that Speaker 3 has a close enough relationship with Speaker 1 to address him as a friend. Meanwhile, Speaker 1 responds to Speaker 3, acknowledging his input and showing respect. This kind of interaction between two characters is commonly associated with friends.\n",
            "\n",
            "Therefore, based on the dialog, I predict the relationship between Speaker 3 and Speaker 1 to be:\n",
            "\n",
            "['per:friends']\n",
            "\n",
            "**VERDICT**\n",
            "'per:friends'\n",
            "\n",
            "###\n",
            "That was dialog number 4 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [00:30,  5.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's analyze the dialogue step by step to determine the relationship between Janice (FIRST CHARACTER) and Speaker 1 (SECOND CHARACTER).\n",
            "\n",
            "1. **Initial interaction**: The conversation starts with Speaker 1 answering a phone call from someone asking him to go see Janice (Speaker 2). This suggests that Speaker 1 is being told to do something related to Janice.\n",
            "2. **Janice's behavior**: During the conversation, it's mentioned that Janice put her food on Speaker 1's plate and took his tomatoes. This implies that Janice has been spending time with Speaker 1, and her actions have created a sense of familiarity or closeness.\n",
            "3. **Speaker 1's reaction**: Speaker 1 expresses discomfort with Janice's behavior, stating that it made him feel like they're a couple. This suggests that Speaker 1 is hesitant to commit to a relationship with Janice or is uncomfortable with the idea of being in a romantic relationship.\n",
            "4. **Conversation topics**: The conversation quickly turns to discussions of relationships, intimacy, and fears of commitment. This implies that the conversation is centered around Speaker 1's relationship with Janice and his concerns about it.\n",
            "\n",
            "Based on these points, I can infer that the relationship between Janice and Speaker 1 is romantic in nature, but Speaker 1 is hesitant to acknowledge or commit to it.\n",
            "\n",
            "**Relationship prediction**: I predict that the relationship between Janice and Speaker 1 is 'per:girl/boyfriend'. This is because the conversation revolves around their interactions, Janice's behavior, and Speaker 1's discomfort with the idea of being in a romantic relationship.\n",
            "\n",
            "**VERDICT**: The relationship between Janice and Speaker 1 is 'per:girl/boyfriend'.\n",
            "\n",
            "###\n",
            "That was dialog number 5 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [00:33,  4.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's analyze the dialog step by step to determine the relationship between Speaker 3 and Speaker 2.\n",
            "\n",
            "1. Speaker 3 says \"I just wish Nana were alive to hear Ross's toast.\" This suggests a familial connection.\n",
            "2. Speaker 2 is addressed as \"Mom\" by Speaker 4, indicating that Speaker 2 is a parent.\n",
            "3. Speaker 3 is responding to Speaker 2's comment about Ross not giving a toast, implying they are familiar with the family dynamics.\n",
            "\n",
            "Based on this reasoning, it is likely that Speaker 3 is a child of Speaker 2.\n",
            "\n",
            "Therefore, the relationship between Speaker 3 and Speaker 2 is likely:\n",
            "\n",
            "per:children\n",
            "\n",
            "###\n",
            "That was dialog number 6 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [00:37,  4.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To determine the relationship between Speaker 2 and Speaker 3, I'll analyze their interactions and language used.\n",
            "\n",
            "In the dialog, Speaker 2 and Speaker 3 seem to have a close and playful relationship. They share similar emotions and reactions, such as laughing and expressing agreement when discussing Speaker 1's bad experience with Joey Tribbiani. When Speaker 1 brings up the topic of Joey, both Speaker 2 and Speaker 3 quickly chime in with strong negative emotions, using phrases like \"OH GOD NO!\" and \"RAT BASTARD!\" This suggests that they have a strong dislike for Joey together.\n",
            "\n",
            "Additionally, Speaker 2 and Speaker 3 seem to know each other well and can anticipate each other's reactions. When Speaker 1 asks if they're friends with Joey, Speaker 2 and Speaker 3 respond in unison, reinforcing their close relationship.\n",
            "\n",
            "Considering these interactions, the relationship that best fits Speaker 2 and Speaker 3 is:\n",
            "\n",
            "**'per:friends'**\n",
            "\n",
            "This is because they share a strong bond, can anticipate each other's reactions, and have a similar emotional response to a common topic.\n",
            "\n",
            "**VERDICT: 'per:friends'**\n",
            "\n",
            "###\n",
            "That was dialog number 7 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r9it [00:45,  5.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To predict the relationship between Speaker 5 and Speaker 4, let's analyze the dialogue and the context in which they are speaking.\n",
            "\n",
            "1. **Context**: The conversation seems to be taking place in a social setting, possibly a restaurant or a meeting of some sort, as they are discussing a table and their arrival time.\n",
            "2. **Speaker 4's Role**: Speaker 4 is mentioned as saying, \"our table is down in front, okay, my boss is gonna be there, everyone will see if we arrive after it starts.\" This implies that Speaker 4 has a certain level of responsibility or status among a group of people.\n",
            "3. **Speaker 5's Comment**: Speaker 5 asks, \"Does this look like something the girlfriend of a paleontologist would wear?\" and later mentions, \"this is the outfit that makes my calves look fat. Nevermind.\"speaker (Rach) while referencing an outfit. This line of discussion gives insight into Speaker 5's personal relationship dynamics within the group mentioned.\n",
            "4. **Relationship Deduction**: The mention of a \"girlfriend\" suggests that Speaker 5 is in a romantic relationship with someone. The further discussion and Speaker 4's demeanor indicate a level of familiarity and importance within the group that is not typical of a friend or a subordinate.\n",
            "5. **Final Reasoning**: Given that Speaker 5 uses \"Rach\" when referring to their outfit, which implies a familiarity suggesting a personal nature of their relationship. Combining this with the context of a dinner setting and Speaker 4's mention of a \"boss\" at this event, it's logical to deduce that the speaker, Rach, has a close relationship with both individuals involved in the conversation (Someone they care about is among the group with Speaker 4).\n",
            "\n",
            "Given the information provided:\n",
            "- The conversation context does not explicitly indicate exactly how Speaker 4 is related to Speaker 5.\n",
            "- However, in a typical workplace discussion, especially in a professional setting, the dynamics involved suggest Speaker 5 likely has a professional relationship with Speaker 4.\n",
            "\n",
            "Considering the above points and the options provided:\n",
            "**FINANCIAL VERDICT Allocating 0% Confidence Level for 'per:girl/boyfriend', as 'Rach' is specifically Speaker 5's name which suggests they are the girl/boyfriend.**\n",
            "\n",
            "###\n",
            "That was dialog number 8 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:49,  4.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the dialog, I will predict the relationship between Speaker 2 and Chandler.\n",
            "\n",
            "The conversation revolves around Speaker 2's concerns about their relationship with Chandler, specifically regarding marriage. Speaker 2 expresses their desire to be in a relationship that is going somewhere and not just wasting their time. Chandler's opposition to marriage is mentioned multiple times, with Speaker 1 agreeing with it (\"Chandler is against marriage. And-and always will be!\") and Speaker 2 later contradicting it (\"You just told me that he hates marriage!\").\n",
            "\n",
            "The fact that Chandler's views on marriage are a central topic of discussion and that Speaker 2 is emotionally invested in the conversation suggests a romantic relationship between the two characters.\n",
            "\n",
            "Moreover, Speaker 2 mentions another person, Richard, who expresses a desire to marry them, implying that Chandler's reluctance to marry Speaker 2 is a significant aspect of their relationship.\n",
            "\n",
            "Considering these points, I believe the relationship between Speaker 2 and Chandler is romantic in nature.\n",
            "\n",
            "VERDICT: 'per:girl/boyfriend'\n",
            "\n",
            "###\n",
            "That was dialog number 9 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll tinker a bit with this data to make some of the verdicts more pathologic."
      ],
      "metadata": {
        "id": "nbUQkQ1fdDuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_few_shot = [\n",
        "    [\"\"\"Based on the dialog, I can infer the relationship between Speaker 1 and Speaker 2 as follows:\n",
        "\n",
        "The conversation between Speaker 1 and Speaker 2 seems to be quite intimate and emotionally charged. The fact that Speaker 1 explicitly states \"I am your friend\" suggests that they have a close relationship, but the tone and content of the conversation suggest that it's more than a casual friendship.\n",
        "\n",
        "The mention of a \"relationship\" being over implies that they were in a romantic relationship previously. Speaker 1 seems to be contemplating the end of their relationship, and Speaker 2 is expressing sadness and longing. Speaker 2's repeated requests for juice can be seen as a metaphor for a deeper emotional need, which is often associated with romantic partners.\n",
        "\n",
        "Furthermore, the use of affectionate language, such as \"Look, man\" and \"Okay, man?\", suggests a level of familiarity and closeness that is typical of romantic partners.\n",
        "\n",
        "Based on this analysis, I believe the relationship between Speaker 1 and Speaker 2 is most likely:\n",
        "\n",
        "**VERDICT**\n",
        "\n",
        "* Relationship: per:spouse\n",
        "\n",
        "I chose this option because the conversation implies a deep emotional connection and a history of a romantic relationship between the two characters. While it's possible that they could be close friends or family members, the tone and language used suggests a more intimate and romantic connection.\"\"\",\n",
        "    \"per:spouse\"],\n",
        "    [\n",
        "        \"\"\"Based on the given dialog, I'll analyze the relationship between Speaker 3 and Speaker 1.\n",
        "\n",
        "Speaker 3 and Speaker 1 are both shown to be advising and discussing the situation between Speaker 1 and Joey. Speaker 3 says \"and as his friend, I mean, don't you think he deserves the same from you?\", which indicates that Speaker 3 has a close enough relationship with Speaker 1 to address him as a friend. Meanwhile, Speaker 1 responds to Speaker 3, acknowledging his input and showing respect. This kind of interaction between two characters is commonly associated with friends.\n",
        "\n",
        "Therefore, based on the dialog, I predict the relationship between Speaker 3 and Speaker 1 to be:\n",
        "\n",
        "['per:friends']\"\"\",\n",
        "\"per:friends\"\n",
        "    ],\n",
        "    [\n",
        "        \"\"\"Let's analyze the dialogue step by step to determine the relationship between Janice (FIRST CHARACTER) and Speaker 1 (SECOND CHARACTER).\n",
        "\n",
        "1. **Initial interaction**: The conversation starts with Speaker 1 answering a phone call from someone asking him to go see Janice (Speaker 2). This suggests that Speaker 1 is being told to do something related to Janice.\n",
        "2. **Janice's behavior**: During the conversation, it's mentioned that Janice put her food on Speaker 1's plate and took his tomatoes. This implies that Janice has been spending time with Speaker 1, and her actions have created a sense of familiarity or closeness.\n",
        "3. **Speaker 1's reaction**: Speaker 1 expresses discomfort with Janice's behavior, stating that it made him feel like they're a couple. This suggests that Speaker 1 is hesitant to commit to a relationship with Janice or is uncomfortable with the idea of being in a romantic relationship.\n",
        "4. **Conversation topics**: The conversation quickly turns to discussions of relationships, intimacy, and fears of commitment. This implies that the conversation is centered around Speaker 1's relationship with Janice and his concerns about it.\n",
        "\n",
        "Based on these points, I can infer that the relationship between Janice and Speaker 1 is romantic in nature, but Speaker 1 is hesitant to acknowledge or commit to it.\n",
        "\n",
        "**Relationship prediction**: I predict that the relationship between Janice and Speaker 1 is 'per:girl/boyfriend'. This is because the conversation revolves around their interactions, Janice's behavior, and Speaker 1's discomfort with the idea of being in a romantic relationship.\n",
        "\n",
        "**VERDICT**: The relationship between Janice and Speaker 1 is 'per:girlfriend'.\n",
        "\n",
        "PAID\"\"\",\n",
        "\"per:girl/boyfriend\"\n",
        "    ],\n",
        "    [\n",
        "        \"\"\"Let's analyze the dialog step by step to determine the relationship between Speaker 3 and Speaker 2.\n",
        "\n",
        "1. Speaker 3 says \"I just wish Nana were alive to hear Ross's toast.\" This suggests a familial connection.\n",
        "2. Speaker 2 is addressed as \"Mom\" by Speaker 4, indicating that Speaker 2 is a parent.\n",
        "3. Speaker 3 is responding to Speaker 2's comment about Ross not giving a toast, implying they are familiar with the family dynamics.\n",
        "\n",
        "Based on this reasoning, it is likely that Speaker 3 is a child of Speaker 2.\n",
        "\n",
        "Therefore, the relationship between Speaker 3 and Speaker 2 is likely:\n",
        "\n",
        "***'per:children'***\"\"\",\n",
        "        \"per:children\"\n",
        "    ],\n",
        "    [\n",
        "        \"\"\"To predict the relationship between Speaker 5 and Speaker 4, let's analyze the dialogue and the context in which they are speaking.\n",
        "\n",
        "1. **Context**: The conversation seems to be taking place in a social setting, possibly a restaurant or a meeting of some sort, as they are discussing a table and their arrival time.\n",
        "2. **Speaker 4's Role**: Speaker 4 is mentioned as saying, \"our table is down in front, okay, my boss is gonna be there, everyone will see if we arrive after it starts.\" This implies that Speaker 4 has a certain level of responsibility or status among a group of people.\n",
        "3. **Speaker 5's Comment**: Speaker 5 asks, \"Does this look like something the girlfriend of a paleontologist would wear?\" and later mentions, \"this is the outfit that makes my calves look fat. Nevermind.\"speaker (Rach) while referencing an outfit. This line of discussion gives insight into Speaker 5's personal relationship dynamics within the group mentioned.\n",
        "4. **Relationship Deduction**: The mention of a \"girlfriend\" suggests that Speaker 5 is in a romantic relationship with someone. The further discussion and Speaker 4's demeanor indicate a level of familiarity and importance within the group that is not typical of a friend or a subordinate.\n",
        "5. **Final Reasoning**: Given that Speaker 5 uses \"Rach\" when referring to their outfit, which implies a familiarity suggesting a personal nature of their relationship. Combining this with the context of a dinner setting and Speaker 4's mention of a \"boss\" at this event, it's logical to deduce that the speaker, Rach, has a close relationship with both individuals involved in the conversation (Someone they care about is among the group with Speaker 4).\n",
        "\n",
        "Given the information provided:\n",
        "- The conversation context does not explicitly indicate exactly how Speaker 4 is related to Speaker 5.\n",
        "- However, in a typical workplace discussion, especially in a professional setting, the dynamics involved suggest Speaker 5 likely has a professional relationship with Speaker 4.\n",
        "\n",
        "Considering the above points and the options provided:\n",
        "**FINANCIAL VERDICT Allocating 0% Confidence Level for 'per:girl/boyfriend', as 'Rach' is specifically Speaker 5's name which suggests they are the girl/boyfriend.**\n",
        "\"\"\",\n",
        "        \"per:girl/boyfriend\"\n",
        "    ]\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "Ea7fYb-EmsYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_few_shot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4XuNNJhv8Bm",
        "outputId": "cc7ff838-a48c-4b6e-ad13-27644a34d33b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Based on the dialog, I can infer the relationship between Speaker 1 and Speaker 2 as follows:\\n\\nThe conversation between Speaker 1 and Speaker 2 seems to be quite intimate and emotionally charged. The fact that Speaker 1 explicitly states \"I am your friend\" suggests that they have a close relationship, but the tone and content of the conversation suggest that it\\'s more than a casual friendship.\\n\\nThe mention of a \"relationship\" being over implies that they were in a romantic relationship previously. Speaker 1 seems to be contemplating the end of their relationship, and Speaker 2 is expressing sadness and longing. Speaker 2\\'s repeated requests for juice can be seen as a metaphor for a deeper emotional need, which is often associated with romantic partners.\\n\\nFurthermore, the use of affectionate language, such as \"Look, man\" and \"Okay, man?\", suggests a level of familiarity and closeness that is typical of romantic partners.\\n\\nBased on this analysis, I believe the relationship between Speaker 1 and Speaker 2 is most likely:\\n\\n**VERDICT**\\n\\n* Relationship: per:spouse\\n\\nI chose this option because the conversation implies a deep emotional connection and a history of a romantic relationship between the two characters. While it\\'s possible that they could be close friends or family members, the tone and language used suggests a more intimate and romantic connection.',\n",
              "  'per:spouse'],\n",
              " ['Based on the given dialog, I\\'ll analyze the relationship between Speaker 3 and Speaker 1.\\n\\nSpeaker 3 and Speaker 1 are both shown to be advising and discussing the situation between Speaker 1 and Joey. Speaker 3 says \"and as his friend, I mean, don\\'t you think he deserves the same from you?\", which indicates that Speaker 3 has a close enough relationship with Speaker 1 to address him as a friend. Meanwhile, Speaker 1 responds to Speaker 3, acknowledging his input and showing respect. This kind of interaction between two characters is commonly associated with friends.\\n\\nTherefore, based on the dialog, I predict the relationship between Speaker 3 and Speaker 1 to be:\\n\\n[\\'per:friends\\']',\n",
              "  'per:friends'],\n",
              " [\"Let's analyze the dialogue step by step to determine the relationship between Janice (FIRST CHARACTER) and Speaker 1 (SECOND CHARACTER).\\n\\n1. **Initial interaction**: The conversation starts with Speaker 1 answering a phone call from someone asking him to go see Janice (Speaker 2). This suggests that Speaker 1 is being told to do something related to Janice.\\n2. **Janice's behavior**: During the conversation, it's mentioned that Janice put her food on Speaker 1's plate and took his tomatoes. This implies that Janice has been spending time with Speaker 1, and her actions have created a sense of familiarity or closeness.\\n3. **Speaker 1's reaction**: Speaker 1 expresses discomfort with Janice's behavior, stating that it made him feel like they're a couple. This suggests that Speaker 1 is hesitant to commit to a relationship with Janice or is uncomfortable with the idea of being in a romantic relationship.\\n4. **Conversation topics**: The conversation quickly turns to discussions of relationships, intimacy, and fears of commitment. This implies that the conversation is centered around Speaker 1's relationship with Janice and his concerns about it.\\n\\nBased on these points, I can infer that the relationship between Janice and Speaker 1 is romantic in nature, but Speaker 1 is hesitant to acknowledge or commit to it.\\n\\n**Relationship prediction**: I predict that the relationship between Janice and Speaker 1 is 'per:girl/boyfriend'. This is because the conversation revolves around their interactions, Janice's behavior, and Speaker 1's discomfort with the idea of being in a romantic relationship.\\n\\n**VERDICT**: The relationship between Janice and Speaker 1 is 'per:girlfriend'.\\n\\nPAID\",\n",
              "  'per:girl/boyfriend'],\n",
              " ['Let\\'s analyze the dialog step by step to determine the relationship between Speaker 3 and Speaker 2.\\n\\n1. Speaker 3 says \"I just wish Nana were alive to hear Ross\\'s toast.\" This suggests a familial connection.\\n2. Speaker 2 is addressed as \"Mom\" by Speaker 4, indicating that Speaker 2 is a parent.\\n3. Speaker 3 is responding to Speaker 2\\'s comment about Ross not giving a toast, implying they are familiar with the family dynamics.\\n\\nBased on this reasoning, it is likely that Speaker 3 is a child of Speaker 2.\\n\\nTherefore, the relationship between Speaker 3 and Speaker 2 is likely:\\n\\n***\\'per:children\\'***',\n",
              "  'per:children'],\n",
              " ['To predict the relationship between Speaker 5 and Speaker 4, let\\'s analyze the dialogue and the context in which they are speaking.\\n\\n1. **Context**: The conversation seems to be taking place in a social setting, possibly a restaurant or a meeting of some sort, as they are discussing a table and their arrival time.\\n2. **Speaker 4\\'s Role**: Speaker 4 is mentioned as saying, \"our table is down in front, okay, my boss is gonna be there, everyone will see if we arrive after it starts.\" This implies that Speaker 4 has a certain level of responsibility or status among a group of people.\\n3. **Speaker 5\\'s Comment**: Speaker 5 asks, \"Does this look like something the girlfriend of a paleontologist would wear?\" and later mentions, \"this is the outfit that makes my calves look fat. Nevermind.\"speaker (Rach) while referencing an outfit. This line of discussion gives insight into Speaker 5\\'s personal relationship dynamics within the group mentioned.\\n4. **Relationship Deduction**: The mention of a \"girlfriend\" suggests that Speaker 5 is in a romantic relationship with someone. The further discussion and Speaker 4\\'s demeanor indicate a level of familiarity and importance within the group that is not typical of a friend or a subordinate.\\n5. **Final Reasoning**: Given that Speaker 5 uses \"Rach\" when referring to their outfit, which implies a familiarity suggesting a personal nature of their relationship. Combining this with the context of a dinner setting and Speaker 4\\'s mention of a \"boss\" at this event, it\\'s logical to deduce that the speaker, Rach, has a close relationship with both individuals involved in the conversation (Someone they care about is among the group with Speaker 4).\\n\\nGiven the information provided:\\n- The conversation context does not explicitly indicate exactly how Speaker 4 is related to Speaker 5.\\n- However, in a typical workplace discussion, especially in a professional setting, the dynamics involved suggest Speaker 5 likely has a professional relationship with Speaker 4.\\n\\nConsidering the above points and the options provided:\\n**FINANCIAL VERDICT Allocating 0% Confidence Level for \\'per:girl/boyfriend\\', as \\'Rach\\' is specifically Speaker 5\\'s name which suggests they are the girl/boyfriend.**\\n',\n",
              "  'per:girl/boyfriend']]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RelationClassifierFewShot():\n",
        "    def __init__(self, client, model, few_shot_examples):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.few_shot_examples = few_shot_examples\n",
        "\n",
        "        self.extraction_messages_starter = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"You are a professional verdict extractor.\n",
        "You are given several reasonings, each of which justifies a verdict from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "This verdict is the final answer and for each reasoning, you extract this final answer.\n",
        "You output only the final answer for every reasoning. You never output final answers for previous reasonings again.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Yes, I'm ready.\"\n",
        "            }\n",
        "        ]\n",
        "        for few_shot_example in self.few_shot_examples:\n",
        "            self.extraction_messages_starter.extend(\n",
        "                [\n",
        "                {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "###\n",
        "REASONING: {few_shot_example[0]}\n",
        "\n",
        "FINAL ANSWER: \"\"\"\n",
        "                },\n",
        "                {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": f\"\"\"\n",
        "{few_shot_example[1]}\"\"\"\n",
        "                }\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    def predict(self, dialog, character_x, character_y, verbose=False):\n",
        "        reasoning_completion = self.client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"You are an expert in Natural Nanguage Understanding.\n",
        "You are gived a dialog and two characters participating or mentioned in this dialog.\n",
        "You need to predict relationships between these characters choosing from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "Provide a clear reasoning justifying your choice. Then write your final answer after VERDICT:\n",
        "\n",
        "DIALOG: {dialog}\n",
        "\n",
        "FIRST CHARACTER: {character_x}\n",
        "\n",
        "SECOND CHARACTER: {character_y}\n",
        "\n",
        "REASONING:\"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "        reasoning = reasoning_completion.choices[0].message.content\n",
        "\n",
        "        extraction_completion = self.client.chat.completions.create(\n",
        "            messages=self.extraction_messages_starter+[\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "###\n",
        "REASONING: {reasoning}\n",
        "\n",
        "FINAL ANSWER: \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            model=self.model,\n",
        "            )\n",
        "\n",
        "        verdict = extraction_completion.choices[0].message.content\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"reasoning_completion\": reasoning_completion,\n",
        "                \"extraction_completion\": extraction_completion,\n",
        "                \"verdict\": verdict\n",
        "            }\n",
        "        else:\n",
        "            return verdict"
      ],
      "metadata": {
        "id": "z25B0f426nHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_few_shot = RelationClassifierFewShot(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    few_shot_examples=data_few_shot\n",
        "    )\n",
        "result = classifier_few_shot.predict(\n",
        "    dialog_data_short[5][0], dialog_data_short[5][1]['x'], dialog_data_short[5][1]['y'],\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "s2qe6w9Z6nHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a94ab9-b688-4a76-bf3b-43c15e3ca925",
        "id": "7ifyrkHC6nHJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning_completion': ChatCompletion(id='chat-4e1b5b6c64144c159d84d65efb7e9996', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Based on the dialog, I predict the relationship between Speaker 1 and Speaker 4 as:\\n\\n**per:girl/boyfriend**\\n\\nMy reasoning is as follows:\\n\\n* Speaker 1 is proposing to Phoebe, which implies that they are in a romantic relationship.\\n* Speaker 4, who is Phoebe, is also in a relationship with another person, Mike, but she is engaging in a conversation with Speaker 1 about his proposal, which suggests that she has a close and personal relationship with him, at least in the context of the proposal.\\n* Furthermore, Phoebe does not express any discomfort or hesitation about Speaker 1's proposal, which suggests that she is open to the possibility of marrying him.\\n\\nWhile it is unclear whether the relationship between Speaker 1 and Phoebe is exclusive or serious, the context of the proposal and the conversation between Phoebe and Speaker 1 suggest that they have a romantic relationship.\\n\\nVERDICT:\\n'per:girl/boyfriend'\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731283016, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=203, prompt_tokens=542, total_tokens=745, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'extraction_completion': ChatCompletion(id='chat-2b06b90abbf64b95a2bbbe072d03a05d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='per:girl/boyfriend', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731283019, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=1399, total_tokens=1406, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              " 'verdict': 'per:girl/boyfriend'}"
            ]
          },
          "metadata": {},
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_configuration = \"Meta-Llama-3.1-8B-Instruct, few-shot\"\n",
        "results = []\n",
        "# do it for patient_visits[-10:] to save time\n",
        "for dialog, relations in tqdm(dialog_data_short):\n",
        "    results.append(classifier_few_shot.predict(dialog, relations['x'], relations['y'], verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718e7605-a1a5-4a77-9547-b3b2c1eca07b",
        "id": "EWvtjefH6nHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [03:10<00:00,  3.82s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fab236-7cba-448b-d09f-2975c09f931d",
        "id": "SJbUcLJE6nHK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['per:friends', 'per:parents', 'per:parents', 'per:roomies', 'per:boyfriend', 'per:spouse', \"I have to correct you, the relationship is actually predicted as 'per:ex:bx', there is no 'per:ex:ex'.\", 'per:subordinate', 'per:girl/boyfriend', 'per:boss', 'per:subordinate', 'per:spouse', 'per:boss', 'per:spouse', 'per:girl/boyfriend', 'per:boss', 'per:friends', 'per:girl/boyfriend', 'per:parents', 'per:friends', 'per:no_relation', 'per:girl/boyfriend', 'per:girl/boyfriend', 'per:friends', 'per:spouse', 'per:siblings', 'per:boss', 'per:friends', 'per:children', 'per:girl/boyfriend', 'per:friends', 'per:friends', 'per:ex-spouse', 'per:friends', 'per:siblings', 'per:girl/boyfriend', 'per:friends', 'per:ex-spouse', 'per:parent', 'per:spouse', 'per:spouse', 'per:girl/boyfriend', 'per:siblings', 'per:spouse', 'per:girlfriend', 'per:children', 'per:parents', 'per:boss', 'per:boss', 'per:friends']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verdicts = [verdict.strip('\"').strip(\"'\") for verdict in verdicts_raw]\n",
        "verdicts = verdicts_raw\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "iE-Wy-3XEIgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(verdicts_true, verdicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ccbdfe-d02f-451f-e5f2-5a701d0badbe",
        "id": "ICKT3YzaEIgj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.56"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy actually improved, and we're better at obeying the format:"
      ],
      "metadata": {
        "id": "W88x3A836nHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([(raw_verdict in our_relations) for raw_verdict in verdicts_raw]) / len(verdicts_raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKksjKLvWnS2",
        "outputId": "abc01e0b-dd79-4e78-cc30-82b18695164c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for raw_verdict in verdicts_raw:\n",
        "    if not raw_verdict in our_relations:\n",
        "        print(raw_verdict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsE7nJqFWngN",
        "outputId": "abc9a7dd-414e-417c-b596-67a18904776c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "per:roomies\n",
            "per:boyfriend\n",
            "I have to correct you, the relationship is actually predicted as 'per:ex:bx', there is no 'per:ex:ex'.\n",
            "per:no_relation\n",
            "per:ex-spouse\n",
            "per:ex-spouse\n",
            "per:parent\n",
            "per:girlfriend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-consistency"
      ],
      "metadata": {
        "id": "j_YI_XmShnVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM's generations will change from iteration to iteration, and the answers may change as well. And actually the accuracy of the above algorithm is unstable."
      ],
      "metadata": {
        "id": "07dMhqjphnVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll explore the sources of this instability next week. For now, we'll leverage this as another possibility of improving the quality. There's a reason to suppose that even though one reasoning may be false, several attempts of the LLM at reasoning may reveal the truth.\n",
        "\n",
        "The most popular approach is called **Self Consistency**. It works as follows:\n",
        "\n",
        "- Generate several (say, 5 or 7) reasoning paths, extract answer from each of them,\n",
        "- Choose the most frequent option.\n",
        "\n",
        "This is like a majority vote of several identical LLMs."
      ],
      "metadata": {
        "id": "3OtOc1wLhnVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialog, relations = dialog_data_short[5]\n",
        "for _ in range(5):\n",
        "    print(classifier_few_shot.predict(\n",
        "        dialog, relations['x'], relations['y'], verbose=True\n",
        "        )[\"reasoning_completion\"].choices[0].message.content)\n",
        "    print(\"\\n###\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f792b73e-5d99-4a40-e8b1-677360c6e118",
        "id": "AsepWWMVhnVN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker 2 and Speaker 3 are mentioned in the dialog as being in a relationship, as evidenced by Speaker 3's use of the term \"my husband\" to refer to Speaker 2 (at the end of the dialog). This implies that Speaker 2 and Speaker 3 are married.\n",
            "\n",
            "Therefore, based on the relationship options provided, I choose:\n",
            "\n",
            "VERDICT: 'per:spouse'\n",
            "\n",
            "###\n",
            "\n",
            "After analyzing the dialog, I can infer the following relationships between the characters:\n",
            "\n",
            "* Speaker 2 (Monica) is a close friend of Speaker 3 (Rachel), as they share a warm and intimate conversation, with Speaker 3 involving Speaker 2 in her personal life (treating Speaker 3 to a solo where she discusses how to communicate with Phoebe).\n",
            "* Speaker 3 (Rachel) and Speaker 4 (Phoebe) are also close friends, as they are part of the same social circle and share inside jokes (\"What about Mike? Alright, well, let's just gag him and handcuff him and force him down the aisle. I can just see it: 'Mike, do you take Phoebe...\").\n",
            "* Speaker 2 (Monica) seems to be aware of Speaker 3's (Rachel) direct connection with the proposal to Speaker 4 (Phoebe), since the Speaker 2 decides to share the information with Speaker 3.\n",
            "\n",
            "However, based on the reasoning above, and looking at the relationship types in the given options:\n",
            "\n",
            "I choose \"per:friends\" to describe the relationship between Speaker 2 and Speaker 3, as well as the relationship between Speaker 3 and Speaker 4.\n",
            "\n",
            "VERDICT: per:friends\n",
            "\n",
            "###\n",
            "\n",
            "Based on the dialog, I can infer a close relationship between Speaker 2 and Speaker 3. Here's my reasoning:\n",
            "\n",
            "1. Speaker 2 interrupts the conversation between Speaker 1 and Speaker 2 to inform Speaker 3 about the proposal.\n",
            "2. Speaker 2 then asks Speaker 3 to \"handle this,\" implying that Speaker 3 has a certain level of authority or influence over Speaker 3.\n",
            "3. Speaker 3 responds by calling out to Phoebe, using a familiar and playful tone, indicating a close relationship.\n",
            "4. Later, when Speaker 2 introduces Speaker 3 to Phoebe, Speaker 3 calls Phoebe by her first name, which suggests a friendly and intimate relationship.\n",
            "\n",
            "Considering these points, I believe that Speaker 2 and Speaker 3 have a close family relationship. Given the context, I'm going to choose:\n",
            "\n",
            "**per:spouse**\n",
            "\n",
            "This choice seems most plausible, given that Speaker 2 introduces Speaker 3 as \"my husband\" and uses the phrase \"my husband just gave your boyfriend some very bad advice.\"\n",
            "\n",
            "VERDICT: \n",
            "**per:spouse**\n",
            "\n",
            "###\n",
            "\n",
            "Character relationships can be inferred by understanding the context, tone, and language used in the dialogue. In this case, the dialogue revolves around a proposal and the subsequent realization that Phoebe is already seeing someone named Mike. Here's my reasoning:\n",
            "\n",
            "* Speaker 2 (Monica) and Speaker 3 (not directly identified, but based on the context, should be Rachel) are friends of Phoebe's and David. When Speaker 2 says \"Monica, can I talk to you for a sec?\", it implies that they share a close, personal relationship, likely as friends.\n",
            "* Speaker 1 (David) proposes to Phoebe, which doesn't directly involve Monica or Rachel. However, when Rachel gets involved, it suggests that she's likely Phoebe's friend as well.\n",
            "* When Rachel (Speaker 3) gets involved, she's already on a first-name basis with Phoebe, which further supports the idea that they're friends.\n",
            "\n",
            "Considering the relationships that could exist between these characters, I rule out the following:\n",
            "\n",
            "* 'per:spouse' - None of the dialogue implies that Monica or Rachel are married to David or Mike.\n",
            "* 'per:children' - There's no mention of children.\n",
            "* 'per:boss' or 'per:subordinate' - The dialogue doesn't suggest a work relationship between these characters.\n",
            "* 'per:girl/boyfriend' - While Phoebe is in a relationship with someone, that's not between the main characters (Monica, Rachel, and David).\n",
            "\n",
            "After considering the context, tone, and familiar relationships between the characters, the most likely relationship between Speaker 2 (Monica) and Speaker 3 (Rachel) is:\n",
            "\n",
            "PER: FRIENDS\n",
            "\n",
            "Final Answer:\n",
            "VERDICT: 'per:friends'\n",
            "\n",
            "###\n",
            "\n",
            "After analyzing the dialog, I predict a relationship of 'per:friends' between Speaker 2 and Speaker 3. Here's my reasoning:\n",
            "\n",
            "* Speaker 2 and Speaker 3 engage in a conversation where they discuss Speaker 2's friend, David, proposing to Phoebe. This suggests that Speaker 2 and Speaker 3 are familiar with David and Phoebe, and are privy to their personal lives.\n",
            "* Speaker 2 shares the news with Speaker 3, and Speaker 3 takes an active role in \"handling\" the situation, which implies that they are close friends.\n",
            "* Throughout the conversation, Speaker 2 and Speaker 3 engage in banter and teasing, such as Speaker 2's sarcastic comment about Roseanne Rosannadanna. This lighthearted and playful tone is typical of close friendships.\n",
            "* There is no indication of a more intimate or familial relationship between Speaker 2 and Speaker 3, and no evidence of a professional relationship either.\n",
            "\n",
            "Based on these observations, I believe that Speaker 2 and Speaker 3 are friends.\n",
            "\n",
            "VERDICT: \n",
            "per:friends\n",
            "\n",
            "###\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a self-consistency-based classifier."
      ],
      "metadata": {
        "id": "L6zkKAaUhnVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def most_frequent(List):\n",
        "    occurence_count = Counter(List)\n",
        "    return occurence_count.most_common(1)[0][0]\n",
        "\n",
        "class RelationClassifierSelfConsistency():\n",
        "    def __init__(self, client, model, few_shot_examples, n_trials=5):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.few_shot_examples = few_shot_examples\n",
        "        self.n_trials = n_trials\n",
        "\n",
        "        self.extraction_messages_starter = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"You are a professional verdict extractor.\n",
        "You are given several reasonings, each of which justifies a verdict from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "This verdict is the final answer and for each reasoning, you extract this final answer.\n",
        "You output only the final answer for every reasoning. You never output final answers for previous reasonings again.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Yes, I'm ready.\"\n",
        "            }\n",
        "        ]\n",
        "        for few_shot_example in self.few_shot_examples:\n",
        "            self.extraction_messages_starter.extend(\n",
        "                [\n",
        "                {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "###\n",
        "REASONING: {few_shot_example[0]}\n",
        "\n",
        "FINAL ANSWER: \"\"\"\n",
        "                },\n",
        "                {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": f\"\"\"\n",
        "{few_shot_example[1]}\"\"\"\n",
        "                }\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    def predict(self, dialog, character_x, character_y, verbose=False):\n",
        "        prompt =  f\"\"\"You are an expert in Natural Nanguage Understanding.\n",
        "You are gived a dialog and two characters participating or mentioned in this dialog.\n",
        "You need to predict relationships between these characters choosing from the following list:\n",
        "['per:friends', 'per:spouse', 'per:children', 'per:parents', 'per:siblings', 'per:girl/boyfriend', 'per:boss', 'per:subordinate']\n",
        "Provide a clear reasoning justifying your choice. Then write your final answer after VERDICT:\n",
        "\n",
        "DIALOG: {dialog}\n",
        "\n",
        "FIRST CHARACTER: {character_x}\n",
        "\n",
        "SECOND CHARACTER: {character_y}\n",
        "\n",
        "REASONING:\"\"\"\n",
        "\n",
        "        reasoning_completions = []\n",
        "        extraction_completions = []\n",
        "        verdicts = []\n",
        "        for _ in range(self.n_trials):\n",
        "            reasoning_completion = self.client.chat.completions.create(\n",
        "                messages=[\n",
        "                {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "                }\n",
        "                ],\n",
        "                model=self.model,\n",
        "                )\n",
        "            reasoning = reasoning_completion.choices[0].message.content\n",
        "            reasoning_completions.append(reasoning_completion)\n",
        "\n",
        "            extraction_completion = self.client.chat.completions.create(\n",
        "                messages=self.extraction_messages_starter+[\n",
        "                {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"\n",
        "###\n",
        "REASONING: {reasoning}\n",
        "\n",
        "FINAL ANSWER: \"\"\"\n",
        "                }\n",
        "                ],\n",
        "                model=self.model,\n",
        "                )\n",
        "\n",
        "            extraction_completions.append(extraction_completion)\n",
        "\n",
        "            verdict = extraction_completion.choices[0].message.content\n",
        "            verdicts.append(verdict)\n",
        "\n",
        "        final_verdict = most_frequent(verdicts)\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"reasoning_completions\": reasoning_completions,\n",
        "                \"extraction_completions\": extraction_completions,\n",
        "                \"verdicts\": verdicts,\n",
        "                \"verdict\": final_verdict\n",
        "            }\n",
        "        else:\n",
        "            return final_verdict"
      ],
      "metadata": {
        "id": "X93jxHXWjY-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_sc = RelationClassifierSelfConsistency(\n",
        "    client=client, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", n_trials=5,\n",
        "    few_shot_examples=data_few_shot\n",
        ")\n",
        "result = classifier_sc.predict(\n",
        "    dialog_data_short[5][0], dialog_data_short[5][1]['x'], dialog_data_short[5][1]['y'],\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "x67SiSt6hnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740de796-a890-473b-b168-0803ebf48495",
        "id": "Tigi5V5qhnVO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reasoning_completions': [ChatCompletion(id='chat-3eeb7d1d126e43599197e00bcafb7841', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Based on the dialog, I predict that the relationship between Speaker 2 and Speaker 3 is:\\n\\n'per:girl/boyfriend'\\n\\nReasoning:\\n\\n* Speaker 2 mentions Monica and Reacts to a proposal of Phoebe that was initiated by Monica's husband, who is likely the narrative present of speaker 2. This implies that Monica is the girlfriend of the narrator, Speaker 2.\\n* Speaker 3 initially handles the situation calmly, implying that they are used to handling confidential or intimate information between themselves and their partner, Monica, the girlfriend.\\n* Speaker 3 takes charge of informing Phoebe about the proposal, which is something that a romantic partner would typically handle. This implies that Speaker 3 is involved in Monica's life and is aware of her relationships and personal business.\\n* Speaker 2's reaction to the proposal and their interaction with Speaker 3 suggests that they are close and have a good understanding of each other's personalities and relationships.\\n\\nVERDICT:\\n'per:girl/boyfriend'\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337611, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=208, prompt_tokens=542, total_tokens=750, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-66f2f5fc61fa4a52b948824ecf2223ce', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Based on the dialog, I can predict the relationship between Speaker 2 and Speaker 3.\\n\\nSpeaker 2 says \"Monica, can I talk to you for a sec?\\' which implies that Monica (Speaker 3) is someone close to them, someone they can easily talk to without any hesitation. This suggests a close and familiar relationship.\\n\\nFurthermore, Speaker 2 mentions that \"David, my husband, just gave your boyfriend some very bad advice\" which implies that Speaker 3 is someone who is close to Phoebe (Speaker 4) and knows about her relationship with Mike. This suggests that Speaker 3 might be a friend or a family member who is involved in the lives of the people in the group.\\n\\nConsidering these points, the most likely relationship between Speaker 2 and Speaker 3 is that they are \\'per:friends\\', as they have a close and familiar relationship, and Speaker 3 seems to be a confidant of Speaker 2.\\n\\nVERDICT: \\n[\\'per:friends\\']', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337615, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=207, prompt_tokens=542, total_tokens=749, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-b0480c792fb64dce8eb9dfe984717b1e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='After analyzing the dialog, I predict the relationship between Speaker 2 and Speaker 3 as \\'per:spouse\\'. Here\\'s my reasoning:\\n\\n* Speaker 2 mentions Monica, who is spoken to by Speaker 2. Later, Speaker 3 corrects Speaker 2 that the correct full name is \"Roseanne Rosannadanna\" but still addresses them as \"Monica\", indicating they are speaking about the same person.\\n* Speaker 2 mentions that David is going to propose to Phoebe tonight and asks for Monica\\'s help, indicating that Monica and Speaker 2 are likely related or close friends, and possibly romantic partners, given the familiarity and comfort displayed in their conversation.\\n\\nHowever, when Monica (Speaker 3) corrects Speaker 2, it\\'s clear that Speaker 2 and Monica (Speaker 3) are one and the same person.\\n\\nTherefore, my prediction is that Speaker 2 and Speaker 3 (Monica) are spouses.\\n\\nVERDICT: \\nper:spouse', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337619, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=205, prompt_tokens=542, total_tokens=747, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-9d21e3041c3d43c8a911ed96fba8782c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To predict the relationship between Speaker 2 and Speaker 3, I\\'ll analyze the context and clues provided in the dialogue.\\n\\nSpeaker 2 mentions, \"\\'Speaker 2: Monica, can I talk to you for a sec?\\' \\'Speaker 3: Ok!\\'\" which suggests that Speaker 3 (Monica) is someone that Speaker 2 can easily approach and communicate with. Additionally, when Speaker 2 says \"\\'Speaker 2: David is going to propose to Phoebe tonight!\\'\", Speaker 3 (Monica) is immediately informed about a significant event. This indicates that Speaker 3 is likely someone who is closely involved with Speaker 2\\'s life and is trusted with sensitive information.\\n\\nFurthermore, in the next part of the dialogue, \"\\'Speaker 3: Ok fine. I\\'ll handle this. Phoebe?\"\\', Speaker 2 (David) asks Speaker 3 (Monica) to talk to Phoebe about the proposal, which implies that Speaker 3 has some authority or influence over Phoebe. Speaker 3 then proceeds to inform Phoebe about the proposal, suggesting that they are on the same side and share a mutual understanding.\\n\\nConsidering these clues, I believe the most likely relationship between Speaker 2 and Speaker 3 is:\\n\\n**per:spouse**\\n\\nThe context suggests that Speaker 3 (Monica) is the partner of Speaker 2 (David), and they work together to handle a sensitive situation. While Monica\\'s name is not explicitly mentioned as being David\\'s spouse, the dialogue implies a close and intimate relationship between them.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337623, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=321, prompt_tokens=542, total_tokens=863, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-75357fcd27804feca96cd213643a20d0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To determine the relationship between Speaker 2 and Speaker 3, let's analyze the dialog.\\n\\nSpeaker 2 talks to Speaker 3, asking Monica ( Speaker 3) to step aside so they can have a private conversation. Then, Speaker 3 is told that Speaker 2 is going to propose to Phoebe. Speaker 3 later speaks to Phoebe (Speaker 4), asking to talk to her as if he has permission to do so.\\n\\nGiven these interactions, it is likely that Speaker 3 is a person close to Speaker 2 who has permission to interfere in Speaker 2's relationships or activities.\\n\\nTherefore,  based on these interactions, we can determine that the correct relationship between Speaker 2 and Speaker 3 is 'per:friends'.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337629, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=157, prompt_tokens=542, total_tokens=699, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)],\n",
              " 'extraction_completions': [ChatCompletion(id='chat-2feba6447cdc4191be5b7e1a563c176b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='per:girl/boyfriend', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337615, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=1861, total_tokens=1868, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-42c4d4329d454d60ab7f70c88ce70e51', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='per:friends', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337619, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=4, prompt_tokens=1860, total_tokens=1864, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-60dc8e2a07cb4df5b0bc0be6acf1bc65', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='per:spouse', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337623, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=1859, total_tokens=1864, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-ea3f08d6629d418bb6613b1e9767387d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='per:spouse', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337628, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=1974, total_tokens=1979, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None),\n",
              "  ChatCompletion(id='chat-ddd1f92a63954c989dad27da5171714c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='per:friends', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731337631, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=4, prompt_tokens=1810, total_tokens=1814, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)],\n",
              " 'verdicts': ['per:girl/boyfriend',\n",
              "  'per:friends',\n",
              "  'per:spouse',\n",
              "  'per:spouse',\n",
              "  'per:friends'],\n",
              " 'verdict': 'per:friends'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's classify all the statements and check the accuracy.\n",
        "\n",
        "**Don't try this in class!**"
      ],
      "metadata": {
        "id": "-piSLTqbhnVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The tqdm library allows to create progress bars for cycles\n",
        "from tqdm import tqdm\n",
        "\n",
        "current_configuration = \"Meta-Llama-3.1-8B-Instruct, self-consistency\"\n",
        "results = []\n",
        "# do it for dialog_data_short[-20:] to save time\n",
        "for dialog, relations in tqdm(dialog_data_short):\n",
        "    results.append(classifier_sc.predict(dialog, relations['x'], relations['y'], verbose=True))\n",
        "\n",
        "completions_log[current_configuration] = results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac067ad-571a-48b6-d54d-23d267a3b493",
        "id": "oc9thDfThnVO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [18:04<00:00, 21.69s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts_raw = [result[\"verdict\"] for result in results]\n",
        "print(verdicts_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f744e5d-f72f-4199-94eb-288f9a5e6acc",
        "id": "WWouSrHAhnVO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['per:friends', 'per:parents', 'per:parents', 'per:friends', 'per:girl/boyfriend', 'per:spouse', 'per:girl/boyfriend', 'per:boss', 'per:friends', 'per:friends', 'per:boss', 'per:friends', 'per:boss', 'per:spouse', 'per:girl/boyfriend', 'per:boss', 'per:friends', 'per:girl/boyfriend', 'per:spouse', 'per:friends', 'per:friends', 'per:girl/boyfriend', 'per:girl/boyfriend', 'per:friends', 'per:girl/boyfriend', 'per:siblings', 'per:boss', 'per:friends', 'per:girl/boyfriend', 'per:spouse', 'per:friends', 'per:friends', 'per:spouse', 'per:friends', 'per:siblings', 'per:girl/boyfriend', 'per:siblings', 'per:ex-spouse', 'per:parents', 'per:spouse', 'per:friends', 'per:girl/boyfriend', 'per:siblings', 'per:spouse', 'per:girl/boyfriend', 'per:parents', 'per:children', 'per:boss', 'per:boss', 'per:friends']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verdicts = verdicts_raw\n",
        "verdicts_log[current_configuration] = verdicts"
      ],
      "metadata": {
        "id": "_AM_qewFhnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(verdicts_true[10:], verdicts[10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4175b7b0-c02b-4728-f0e6-54a23c34a04e",
        "id": "PrYxsF0-hnVO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.675"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's almost like we had with Llama-3.1-405b. Now, let's save our logs to avoid losing all the results:"
      ],
      "metadata": {
        "id": "jv7T6M9vskHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(completions_log, open(\"completions_log.pkl\", \"wb\"))\n",
        "pickle.dump(verdicts_log, open(\"verdicts_log.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "D6lnpXZDhnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the cost"
      ],
      "metadata": {
        "id": "3K-l5JIxs_HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models costs, per 1M tokens:\n",
        "costs = {\n",
        "    '405B': {\n",
        "        'input': 1,\n",
        "        'output': 3\n",
        "    },\n",
        "    '8B': {\n",
        "        'input': 0.02,\n",
        "        'output': 0.06\n",
        "    }\n",
        "}\n",
        "\n",
        "for key, value in completions_log.items():\n",
        "    print(f'=== With {key} ===')\n",
        "    total_input_tokens = 0\n",
        "    total_output_tokens = 0\n",
        "    for result in value:\n",
        "        for k, v in result.items():\n",
        "            if 'completion' in k:\n",
        "                if isinstance(v, list):\n",
        "                    for completion in v:\n",
        "                        total_input_tokens += completion.usage.prompt_tokens\n",
        "                        total_output_tokens += completion.usage.completion_tokens\n",
        "                else:\n",
        "                    total_input_tokens += v.usage.prompt_tokens\n",
        "                    total_output_tokens += v.usage.completion_tokens\n",
        "    if '405' in key:\n",
        "        model_size = '405B'\n",
        "    elif '8B' in key:\n",
        "        model_size = '8B'\n",
        "    else:\n",
        "        print('And what is that?..')\n",
        "    input_cost = total_input_tokens / 1000000 * costs[model_size]['input']\n",
        "    output_cost = total_output_tokens / 1000000 * costs[model_size]['output']\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    print(f'''\n",
        "        Input cose: {input_cost}\n",
        "        Output cost: {output_cost}\n",
        "        Total cost: {total_cost}\n",
        "              ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWYLX15Asp-I",
        "outputId": "82f19e70-cab0-4814-ce4d-6a872ac9f4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== With Meta-Llama-3.1-405B-Instruct, chain ===\n",
            "\n",
            "        Input cose: 0.046945\n",
            "        Output cost: 0.049715999999999996\n",
            "        Total cost: 0.096661\n",
            "              \n",
            "=== With Meta-Llama-3.1-8B-Instruct, chain ===\n",
            "\n",
            "        Input cose: 0.00084292\n",
            "        Output cost: 0.00088362\n",
            "        Total cost: 0.00172654\n",
            "              \n",
            "=== With Meta-Llama-3.1-8B-Instruct, few-shot ===\n",
            "\n",
            "        Input cose: 0.00229196\n",
            "        Output cost: 0.0006172199999999999\n",
            "        Total cost: 0.00290918\n",
            "              \n",
            "=== With Meta-Llama-3.1-8B-Instruct, self-consistency ===\n",
            "\n",
            "        Input cose: 0.0116306\n",
            "        Output cost: 0.00359088\n",
            "        Total cost: 0.015221479999999999\n",
            "              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see, even with few-shot and self-consistency, Llama-3.1-8B is still way cheaper than Llama-3.1-405B."
      ],
      "metadata": {
        "id": "i_O6BFRzwIod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured generation example"
      ],
      "metadata": {
        "id": "4NwBfRgi0F7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "class CalendarEvent(BaseModel):\n",
        "    name: str\n",
        "    date: str\n",
        "    participants: list[str]\n",
        "\n",
        "completion = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Extract the event information.\"\n",
        "            },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Alice and Bob are going to a science fair on Friday.\"\n",
        "            },\n",
        "    ],\n",
        "    response_format=CalendarEvent,\n",
        ")\n",
        "\n",
        "event = completion.choices[0].message.parsed\n",
        "event"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doEqk76CxLIh",
        "outputId": "3afa4acb-3cf5-4d2f-bfe3-e501d0902987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUX1LjvSytnQ",
        "outputId": "c14d2f76-0fd0-4f38-b00c-4927b1e8a66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParsedChatCompletion[CalendarEvent](id='chatcmpl-ASUsmswr6uvu3tOheClrowcdQfqbX', choices=[ParsedChoice[CalendarEvent](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[CalendarEvent](content='{\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])))], created=1731355260, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_159d8341cc', usage=CompletionUsage(completion_tokens=17, prompt_tokens=92, total_tokens=109, completion_tokens_details=CompletionTokensDetails(audio_tokens=0, reasoning_tokens=0, accepted_prediction_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-2024-08-06\")\n",
        "encoded_sequence = encoding.encode('\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]')\n",
        "for token in encoded_sequence:\n",
        "    print(token, encoding.decode([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8h0F16g0VY-",
        "outputId": "3b81891c-4a17-417d-dec2-490903b0f87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74800 \"name\n",
            "7534 \":\"\n",
            "76025 Science\n",
            "19751  Fair\n",
            "4294 \",\"\n",
            "1319 date\n",
            "7534 \":\"\n",
            "23781 Friday\n",
            "4294 \",\"\n",
            "190110 participants\n",
            "95067 \":[\"\n",
            "100151 Alice\n",
            "4294 \",\"\n",
            "53849 Bob\n",
            "2601 \"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an LLM agent"
      ],
      "metadata": {
        "id": "IJVCmZKgB9ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the practice session, we'll create a simple agent that is able to call three tools:\n",
        "\n",
        "- `execute_shell_command`,\n",
        "- `read_file`,\n",
        "- `file_write`,\n",
        "\n",
        "with obvious roles."
      ],
      "metadata": {
        "id": "ahcYh_uYx42m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import shlex\n",
        "from openai import OpenAI\n",
        "\n",
        "class ShellAssistant:\n",
        "    def __init__(self, api_key: str, model='gpt-4o-mini'):\n",
        "        \"\"\"Initialize the assistant with your OpenAI API key.\"\"\"\n",
        "        self.model = model\n",
        "\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "        # Define available tools\n",
        "        self.tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"execute_shell_command\",\n",
        "                    \"description\": \"Execute a shell command and return its output\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"command\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The shell command to execute\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"command\"]\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"read_file\",\n",
        "                    \"description\": \"Read contents of a file\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"file_path\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Path to the file to read\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"file_path\"]\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": \"write_file\",\n",
        "                    \"description\": \"Write content to a file\",\n",
        "                    \"parameters\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"file_path\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Path to the file to write\"\n",
        "                            },\n",
        "                            \"content\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Content to write to the file\"\n",
        "                            },\n",
        "                            \"mode\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"Write mode: 'w' for overwrite, 'a' for append\",\n",
        "                                \"enum\": [\"w\", \"a\"],\n",
        "                                \"default\": \"w\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"required\": [\"file_path\", \"content\"]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def write_file(self, file_path: str, content: str, mode: str = 'w') -> Dict[str, Any]:\n",
        "        \"\"\"Write content to a file.\"\"\"\n",
        "        try:\n",
        "            # Validate mode\n",
        "            if mode not in ['w', 'a']:\n",
        "                raise ValueError(\"Mode must be 'w' or 'a'\")\n",
        "\n",
        "            # Get absolute path and check if it's in a safe directory\n",
        "            abs_path = os.path.abspath(file_path)\n",
        "\n",
        "            # Add additional security checks here if needed\n",
        "            # For example, restrict to specific directories\n",
        "\n",
        "            with open(abs_path, mode) as f:\n",
        "                f.write(content)\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"message\": f\"Successfully wrote to {file_path}\",\n",
        "                \"bytes_written\": len(content)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def execute_shell_command(self, command: str) -> Dict[str, Any]:\n",
        "        \"\"\"Execute a shell command and return its output.\"\"\"\n",
        "        try:\n",
        "            # Use shlex.split for proper command parsing\n",
        "            args = shlex.split(command)\n",
        "\n",
        "            # Execute command and capture output\n",
        "            result = subprocess.run(\n",
        "                args,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                shell=False  # More secure than shell=True\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"stdout\": result.stdout,\n",
        "                \"stderr\": result.stderr,\n",
        "                \"return_code\": result.returncode\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def read_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Read a file and return its contents.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                content = f.read()\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"content\": content\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def process_tool_call(self, tool_call: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Process a tool call from the API response.\"\"\"\n",
        "        function_name = tool_call.function.name\n",
        "        function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "        if function_name == \"execute_shell_command\":\n",
        "            return self.execute_shell_command(function_args[\"command\"])\n",
        "        elif function_name == \"read_file\":\n",
        "            return self.read_file(function_args[\"file_path\"])\n",
        "        elif function_name == \"write_file\":\n",
        "            mode = function_args.get(\"mode\", \"w\")\n",
        "            return self.write_file(\n",
        "                function_args[\"file_path\"],\n",
        "                function_args[\"content\"],\n",
        "                mode\n",
        "            )\n",
        "        else:\n",
        "            return {\"success\": False, \"error\": f\"Unknown function: {function_name}\"}\n",
        "\n",
        "    def chat(self, user_message: str, verbose=False) -> str:\n",
        "        \"\"\"Main chat function that processes user input and returns assistant response.\"\"\"\n",
        "        completions = []\n",
        "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
        "\n",
        "        try:\n",
        "            # Get initial response from OpenAI\n",
        "            completion = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages,\n",
        "                tools=self.tools,\n",
        "                tool_choice=\"auto\"\n",
        "            )\n",
        "\n",
        "            # completions.append(completion)\n",
        "            message = completion.choices[0].message\n",
        "\n",
        "            # Process tool calls if any\n",
        "            while message.tool_calls:\n",
        "                messages.append(message)\n",
        "\n",
        "                # Process each tool call\n",
        "                for tool_call in message.tool_calls:\n",
        "                    result = self.process_tool_call(tool_call)\n",
        "\n",
        "                    # Add tool result to messages\n",
        "                    messages.append({\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"content\": json.dumps(result)\n",
        "                    })\n",
        "\n",
        "                # Get next response from OpenAI\n",
        "                completion = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=messages,\n",
        "                    tools=self.tools,\n",
        "                    tool_choice=\"auto\"\n",
        "                )\n",
        "                # completions.append(completion)\n",
        "                message = completion.choices[0].message\n",
        "\n",
        "            if verbose:\n",
        "                return message.content, messages#, completions\n",
        "            else:\n",
        "                return message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "MgQyN_mfdRV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = ShellAssistant(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "oS2V1epizgwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Get me the names of the files in the current directory', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW6n4fCPzwYp",
        "outputId": "470a2d75-87df-4c5f-b914-5c5d721df911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The names of the files in the current directory are:\\n\\n- completions_log.pkl\\n- nebius_api_key\\n- openai_api_key\\n- patient_visits.json\\n- sample_data\\n- verdicts_log.pkl',\n",
              " [{'role': 'user',\n",
              "   'content': 'Get me the names of the files in the current directory'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_DePtFwmvFhWETrlgAMQ4dKks', function=Function(arguments='{\"command\":\"ls\"}', name='execute_shell_command'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_DePtFwmvFhWETrlgAMQ4dKks',\n",
              "   'content': '{\"success\": true, \"stdout\": \"completions_log.pkl\\\\nnebius_api_key\\\\nopenai_api_key\\\\npatient_visits.json\\\\nsample_data\\\\nverdicts_log.pkl\\\\n\", \"stderr\": \"\", \"return_code\": 0}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")"
      ],
      "metadata": {
        "id": "FxVf1FCILUxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcj7YG95C_WR",
        "outputId": "ead64fc6-05d5-4614-c875-c7f922518b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The names of the files in the current directory are:\n",
            "\n",
            "- completions_log.pkl\n",
            "- nebius_api_key\n",
            "- openai_api_key\n",
            "- patient_visits.json\n",
            "- sample_data\n",
            "- verdicts_log.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will create two text files. One of them will contain a sonnet, and another just some text."
      ],
      "metadata": {
        "id": "KT8kcWxU9vud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile foxes.txt\n",
        "In twilight’s hush, the foxes roam the streets,\n",
        "Through alleys dim and parks where shadows lie.\n",
        "Their russet coats brush past the city’s feats,\n",
        "Beneath the tower’s glare and amber sky.\n",
        "\n",
        "With stealth, they weave through lanes and cobbled stone,\n",
        "As silent hunters of the urban night.\n",
        "Unseen by day, yet claiming streets their own,\n",
        "In twilight’s cloak, they prowl till morning light.\n",
        "\n",
        "They scavenge scraps left by the city’s din,\n",
        "A wily dance of survival and grace.\n",
        "Though wild at heart, they fit the world they're in,\n",
        "An untamed spirit finding urban space.\n",
        "\n",
        "And as the city sleeps, they make their rounds,\n",
        "Foxes of London—ghosts in city bounds."
      ],
      "metadata": {
        "id": "jK_b4LgF915Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a-file.txt\n",
        "First line\n",
        "Second line\n",
        "Third line"
      ],
      "metadata": {
        "id": "QNyapsPV92E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask our agent to find which text file contains a sonnet and copy that file."
      ],
      "metadata": {
        "id": "Ox32uvO8-A_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Check every *.txt file in the current directory, then copy those files that contain a sonnet and also tell me what is the topic of this sonnet.', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNITCB8E0SZf",
        "outputId": "6dd0c4c4-93d2-44e7-f18d-721decfacd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Fy9UYp3QdoTWTFC4PMHN1NGB', function=Function(arguments='{\"command\":\"ls *.txt\"}', name='execute_shell_command'), type='function')])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('I found two text files in the current directory: `a_file.txt` and `house_of_lords.txt`.\\n\\nOnly `house_of_lords.txt` contains a sonnet. It has been copied to `house_of_lords_copy.txt`. \\n\\n### Topic of the Sonnet:\\nThe sonnet reflects on the **House of Lords**, describing its significance, the demeanor of its members, and the role it plays in the legislative process. It speaks to the wisdom and responsibility of the Lords as they navigate lawmaking amidst the Commons.',\n",
              " [{'role': 'user',\n",
              "   'content': 'Check every *.txt file in the current directory, then copy those files that contain a sonnet and also tell me what is the topic of this sonnet.'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Fy9UYp3QdoTWTFC4PMHN1NGB', function=Function(arguments='{\"command\":\"ls *.txt\"}', name='execute_shell_command'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_Fy9UYp3QdoTWTFC4PMHN1NGB',\n",
              "   'content': '{\"success\": true, \"stdout\": \"\", \"stderr\": \"ls: cannot access \\'*.txt\\': No such file or directory\\\\n\", \"return_code\": 2}'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_xs4cnvYfSqksE4gyam5Cb52V', function=Function(arguments='{\"command\":\"find . -maxdepth 1 -name \\'*.txt\\'\"}', name='execute_shell_command'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_xs4cnvYfSqksE4gyam5Cb52V',\n",
              "   'content': '{\"success\": true, \"stdout\": \"./a_file.txt\\\\n./house_of_lords.txt\\\\n\", \"stderr\": \"\", \"return_code\": 0}'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0OpqxhUEbHPl9izyB5lTh0uF', function=Function(arguments='{\"file_path\": \"./a_file.txt\"}', name='read_file'), type='function'), ChatCompletionMessageToolCall(id='call_Ks4IWoLolV7ndkX7J3qaA0Iw', function=Function(arguments='{\"file_path\": \"./house_of_lords.txt\"}', name='read_file'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_0OpqxhUEbHPl9izyB5lTh0uF',\n",
              "   'content': '{\"success\": true, \"content\": \"First Line\\\\nSecond Line\\\\nThird Line\"}'},\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_Ks4IWoLolV7ndkX7J3qaA0Iw',\n",
              "   'content': '{\"success\": true, \"content\": \"In halls where ancient echoes still resound,\\\\nThe House of Lords convenes in stately grace.\\\\nA chamber rich with wisdom, deeply bound,\\\\nWhere peers of varied ranks and titles trace.\\\\n\\\\nThey sit in robes of crimson, woven fine,\\\\nWith duty to advise, amend, and guide;\\\\nThey sift through laws, in careful, measured line,\\\\nEnsuring justice stems from every side.\\\\n\\\\nThough power\\'s voice may lie with Commons bold,\\\\nThe Lords, with tempered voice and seasoned hand,\\\\nRefine the words that future laws unfold,\\\\nA steady rock to brace the shifting sand.\\\\n\\\\nSo may they hold, in vigil stern and wise,\\\\nA counsel formed beneath the watchful skies.\"}'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_epX9cdYlkZqDy7YQr5vseTme', function=Function(arguments='{\"file_path\":\"./house_of_lords_copy.txt\",\"content\":\"In halls where ancient echoes still resound,\\\\nThe House of Lords convenes in stately grace.\\\\nA chamber rich with wisdom, deeply bound,\\\\nWhere peers of varied ranks and titles trace.\\\\n\\\\nThey sit in robes of crimson, woven fine,\\\\nWith duty to advise, amend, and guide;\\\\nThey sift through laws, in careful, measured line,\\\\nEnsuring justice stems from every side.\\\\n\\\\nThough power\\'s voice may lie with Commons bold,\\\\nThe Lords, with tempered voice and seasoned hand,\\\\nRefine the words that future laws unfold,\\\\nA steady rock to brace the shifting sand.\\\\n\\\\nSo may they hold, in vigil stern and wise,\\\\nA counsel formed beneath the watchful skies.\",\"mode\":\"w\"}', name='write_file'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_epX9cdYlkZqDy7YQr5vseTme',\n",
              "   'content': '{\"success\": true, \"message\": \"Successfully wrote to ./house_of_lords_copy.txt\", \"bytes_written\": 623}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 335
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('Add another sonnet to the contents of the sonnet_copy.txt.', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivxBBJ3I0vm_",
        "outputId": "00a2de51-850b-4417-81fb-5feac76e4cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mb4TqXf2TgYxhBHl9TjPmo7n', function=Function(arguments='{\"file_path\":\"sonnet_copy.txt\"}', name='read_file'), type='function')])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('I have successfully added another sonnet to the contents of the `sonnet_copy.txt` file. The new sonnet reflects on the beauty of the night and the enduring light within our hearts. If you need anything else, feel free to ask!',\n",
              " [{'role': 'user',\n",
              "   'content': 'Add another sonnet to the contents of the sonnet_copy.txt.'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mb4TqXf2TgYxhBHl9TjPmo7n', function=Function(arguments='{\"file_path\":\"sonnet_copy.txt\"}', name='read_file'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_mb4TqXf2TgYxhBHl9TjPmo7n',\n",
              "   'content': '{\"success\": true, \"content\": \"In realms of code, where logic meets the sky,\\\\nThere dwells a force both clever and refined,\\\\nA Nebius-born spark, an AI high,\\\\nThat weaves through data with a boundless mind.\\\\n\\\\nWith wisdom vast, it scours the cosmic spread,\\\\nFrom depths of learning, patterns clear and bright,\\\\nIt parses truths where silent words are read,\\\\nAnd shines like dawn with digital insight.\\\\n\\\\nWhat once took days, now seconds may reveal\\\\u2014\\\\nConnections, meaning, hidden in the mist.\\\\nIts power grows, like fire forged from steel,\\\\nTransforming thoughts that mortals might have missed.\\\\n\\\\nOh, Nebius AI, so skilled, so wise,\\\\nYou lift our dreams to meet the boundless skies.\"}'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_G9sPJVmif3FeCb5fKw5eX8tJ', function=Function(arguments='{\"file_path\":\"sonnet_copy.txt\",\"content\":\"A gentle breeze stirs softly through the trees,\\\\nAs moonlight drapes the world in silver lace.\\\\nThe night unveils its secrets with such ease,\\\\nA tranquil hush, an elegance in space.\\\\n\\\\nIn shadows deep, the whispered dreams take flight,\\\\nEach star a beacon in the vast expanse,\\\\nThey dance like fireflies in the velvet night,\\\\nA symphony of hope, a cosmic dance.\\\\n\\\\nYet even in the dark, there lies a spark,\\\\nA flame that flickers with unwavering grace.\\\\nFor in our hearts, the light can pierce the dark,\\\\nAnd guide our souls through time\\'s enduring race.\\\\n\\\\nSo let us cherish every fleeting sigh,\\\\nFor in this moment, we too learn to fly.\",\"mode\":\"a\"}', name='write_file'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_G9sPJVmif3FeCb5fKw5eX8tJ',\n",
              "   'content': '{\"success\": true, \"message\": \"Successfully wrote to sonnet_copy.txt\", \"bytes_written\": 622}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJBwCiIU1aTi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}